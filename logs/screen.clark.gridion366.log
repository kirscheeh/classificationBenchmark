[33m
Tip of the Day:	[0m[36mWas man nicht im Kopf hat, das f√ºg' auch keinem andern zu.

[0m
(base) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:06:52 [1;32m/mnt/fass1/kirsten[1;34m][0mconda actva[K[Kivate projectMAIN
(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:07:01 [1;32m/mnt/fass1/kirsten[1;34m][0mcd result/classificationBenchmark/
(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:07:05 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0msnakemake --u se-conda -j 8
[33mProvided cores: 8[0m
[33mRules claiming more threads will be scaled down.[0m
[33mJob counts:
	count	jobs
	1	all
	1	clark
	2[0m
[32m[0m
[32mrule clark:
    input: /mnt/fass1/database/clark_database/targets.txt, /mnt/fass1/kirsten/data/gridion366.fastq
    output: /mnt/fass1/kirsten/result/classification/clark/default/gridion366_default.clark.classification.csv, /mnt/fass1/kirsten/result/classification/clark/default/gridion366_default.clark.classification
    log: /mnt/fass1/kirsten/result/classification/clark/default/gridion366_default.clark.log
    jobid: 1
    benchmark: /mnt/fass1/kirsten/result/classification/benchmarks/default/gridion366_default.clark.benchmark.txt
    wildcards: sample=gridion366, run=default, PATH=/mnt/fass1/kirsten
    threads: 8[0m
[32m[0m
[31mError in job clark while creating output files /mnt/fass1/kirsten/result/classification/clark/default/gridion366_default.clark.classification.csv, /mnt/fass1/kirsten/result/classification/clark/default/gridion366_default.clark.classification.[0m
[31mRuleException:
AttributeError in line 320 of /data/fass1/kirsten/result/classificationBenchmark/Snakefile:
'InputFiles' object has no attribute 'db'
  File "/data/fass1/kirsten/result/classificationBenchmark/Snakefile", line 320, in __rule_clark
  File "/usr/lib/python3.5/string.py", line 191, in format
  File "/usr/lib/python3.5/string.py", line 195, in vformat
  File "/usr/lib/python3.5/string.py", line 235, in _vformat
  File "/usr/lib/python3.5/string.py", line 306, in get_field
  File "/usr/lib/python3.5/concurrent/futures/thread.py", line 55, in run[0m
[31mExiting because a job execution failed. Look above for error message[0m
[33mWill exit after finishing currently running jobs.[0m
[31mExiting because a job execution failed. Look above for error message[0m
(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:07:18 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0mnano Snakefilee 
[?1049h[1;27r(B[m[4l[?7h[?1h=[?1h=[?1h=[39;49m[?25l[39;49m(B[m[H[2J[25;43H(B[0;7m[ Reading File ](B[m[25;42H(B[0;7m[ Read 527 lines ](B[m[H[39;49m(B[0;7m  GNU nano 2.7.4                              File: Snakefile                                        [3;1H(B[m# Snakefile for project work of benchmarking different classification tools regarding their usabilit$[5;1Hconfigfile: "config.yaml"[7d########## VARIABLE DEFINITION[8dDI= dict(config["dataIndex"])[9dPATH = config["path"][10dSAMPLES = "gridion366" #list(config["samples"])[11dTOOLS= 'clark'#ccmetagen centrifuge kraken2 clark kaiju'.split(" ") #list(config["classification"])[12dRUNS='default'#'medianHitLength'#'quals'# 'default medium restrictive'.split()[14dimport scripts.getting[17drule all:[18;5Hinput:[19d# CLASSIFICATION[20;9Hexpand("{path}/result/classification/{tool}/{run}/{sample}_{run}.{tool}.classification", run$[21;1H# GENERATING (comparable) REPORTS[22d#[9Gexpand("{path}/result/classification/{tool}/{run}/{sample}_{run}.{tool}.areport", run=RUNS, $[23;1H#[23;10Hexpand("{path}/result/classification/{tool}/{run}/{sample}_{run}.{tool}.report", run=RUNS, $[24;1H#[9Gexpand("{path}/result/classification/ccmetagen/{sample}.{tool}.intermediate.res", run=RUNS, $[26;1H(B[0;7m^G(B[m Get Help   (B[0;7m^O(B[m Write Out  (B[0;7m^W(B[m Where Is   (B[0;7m^K(B[m Cut Text   (B[0;7m^J(B[m Justify    (B[0;7m^C(B[m Cur Pos    (B[0;7m^Y(B[m Prev Page[27d(B[0;7m^X(B[m Exit[15G(B[0;7m^R(B[m Read File  (B[0;7m^\(B[m Replace    (B[0;7m^U(B[m Uncut Text (B[0;7m^T(B[m To Spell   (B[0;7m^_(B[m Go To Line (B[0;7m^V(B[m Next Page[25d[?12l[?25h[3d#        expand("{path}/result/classification/{tool}/{run}/{sample}_{run}.{tool}.report", run=RUNS, [4d#[9Gexpand("{path}/result/classification/ccmetagen/{sample}.{tool}.intermediate.res", run=RUNS, $[5;1H#       expand("{path}/result/classification/{tool}/default/{sample}_{run}.catbat.bins",tool=TOOLS, $[7;1Hrule diamond_db:[K[8d    input:[K[9;8H[1K faa="/mnt/fass1/kirsten/databases/diamond_all/faa/full_proteome_bacteria_scerevisiae_cneofor$[10;8H[1K map="/mnt/fass1/genomes/new_bacteria/bacteria_blast_db/prot_accession2taxid.txt",[11;8H[1K nodes="/mnt/fass1/kirsten/databases/diamond/nodes.dmp",[K[12;8H[1K names="/mnt/fass1/kirsten/databases/diamond/names.dmp"[K[13;5Houtput:[14;8H[1K "/mnt/fass1/kirsten/databases/diamond_all/nr.dmnd"[15;5Hbenchmark:[16;9H"/mnt/fass1/kirsten/result/classification/benchmarks/diamond.db.benchmark.txt"[17d    conda:[18;5H    "envs/diamond.yaml"[19d    params:[K[20;9H"/mnt/fass1/kirsten/databases/diamond_all/nr"[K[21d    threads: 8[K[22d    shell:[K[23;8H[1K "diamond makedb --in {input.faa} -d {params} --taxonmap {input.map} --taxonnodes {input.node[24d[K[3d[3;8r[8;1H[2S[1;27r[3;8H[1K "diamond makedb --in {input.faa} -d {params} --taxonmap {input.map} --taxonnodes {input.node[5;6Hcentrifuge_db:[7;9Hmap = "/mnt/fass1/kirsten/databases/centrifuge_all/seqid2taxid.map",[8;9Hnodes = "/mnt/fass1/kirsten/databases/centrifuge_all/taxonomy/nodes.dmp",[9;9Hnames ="/mnt/fass1/kirsten/databases/centrifuge_all/taxonomy/names.dmp",[K[10;9Hfaa[10;25Hkirsten/databases/centrifuge_all/fna/full_bacteria_scerevisiae_cneoformans.f$[11;5Houtput:[K[12;8Hfile1= [12;45Hcentrifuge_all/bac_cer_neo.1.cf",[13;5H   file2= "/mnt/fass1/kirsten/databases/centrifuge_all/bac_cer_neo.2.cf",[14;8Hfile3= "/mnt/fass1/kirsten/databases/centrifuge_all/bac_cer_neo.3.cf"[15;5Hthreads: 8[16;5Hbenchmark:[K[17;5H    "/mnt/fass1/kirsten/result/classification/benchmarks/centrifuge.db.benchmark.txt"[18;5Hparams:[K[19;5H    "/mnt/fass1/kirsten/databases/centrifuge_all/bar_cer_neo"[20;5Hconda:[K[21;5H    "envs/centrifuge.yaml"[23;10Hcentrifuge-build -p {threads} --conversion-table {input.map} --taxonomy-tree {input.nodes} [3d[3;10Hcentrifuge-build -p {threads} --conversion-table {input.map} --taxonomy-tree {input.nodes} [5d# creating the project structure[6drule create:[7;5Hshell:[K[8d'python structure.py'[K[9d[K[10ddef get_run(wildcards): #returns the current value of variable/wildcard run[K[11;5Hreturn wildcards.run[12d[K[13ddef get_tool(wildcards): #returns the current value of variable/wildcard run [14;5Hreturn wildcards.tool[K[15d[K[16ddef get_medianHitLength(wildcards):[17;5Hreturn scripts.getting.get_seqLength(str(PATH[0])+"/data/"+str(wildcards.sample)+".fastq")#/2[18d #   elif str(wildcards.tool) == "kaiju":[19;3H#      lengths_default = {'gridion364':201, 'gridion366':194}  [20;4H#     return lengths_default[str(wildcards.sample)]/2[21;5H# return wildcards.sample # scripts.getting.get_seqLength({PATH}+str(wildcards.sample)+".fastq")$[22;1H[K[23drule centrifuge:[K[24;5Hinput:[3drule centrifuge:[K[4;5Hinput:[5;8H[1K fastq = "{PATH}/data/{sample}.fastq"[6d    output:[K[7;5H    files = "{PATH}/result/classification/centrifuge/{run}/{sample}_{run}.centrifuge.classificat$[8;9Hreport= "{PATH}/result/classification/centrifuge/{run}/{sample}_{run}.centrifuge.report"[9;5Hbenchmark:[10;8H[1K "{PATH}/result/classification/benchmarks/{run}/{sample}_{run}.centrifuge.benchmark.txt"[11;5Hthreads: 8[K[12;5Hparams:[13;8H[1K runid=get_run,[K[14;5H    db = DI["centrifuge"],[15d#[15;10Hmedianlength=get_medianHitLength[16d    conda:[K[17;5H   "envs/centrifuge.yaml"[K[18d    run:[K[19d[1K # -q[41m                            [49m(B[mfiles are fastq[K[20d[41m        [49m(B[m# -x[41m                            [49m(B[mindex files[K[21d[41m        [49m(B[m# --report-file[41m         [49m(B[mgenerated report file[K[22d[41m        [49m(B[m# -S[41m                            [49m(B[moutput file[23;8H[1K # -f    [23;29Hquery input files are (multi)fasta[24;5H    # --ignore-quals[3d[8G[1K # -f    [3;29Hquery input files are (multi)fasta[4;5H    # --ignore-quals[5d[K[6;5H    if 'default' in {params.runid}:[7;9H    shell('centrifuge -q -x {params.db} {input.fastq} --report-file {output.report} -S {outp[8;9Helif 'quals' in {params.runid}:[K[9;12H[1K shell('centrifuge -q -x {params.db} {input.fastq} --report-file {output.report} -S {outp$[10;9Helif 'medianHitLength' in {params.runid}:[K[11;13H[1K shell('centrifuge -q -x {params.db} {input.fastq} --report-file {output.report} -S {out$[12;5H    else:[13;9H    print("Centrifuge -- Nothing to be done here:", {params.runid})[14d[K[15drule kraken2:[K[16;5Hinput[17d db = DI['kraken2'],  [18;5H    files = "{PATH}/data/{sample}.fastq"[19;5Houtput:[K[20;8H[1K files = "{PATH}/result/classification/kraken2/{run}/{sample}_{run}.kraken2.classification",[21;8H[1K report= "{PATH}/result/classification/kraken2/{run}/{sample}_{run}.kraken2.report",[22;8H[1K unclassified="{PATH}/result/classification/kraken2/{run}/{sample}_{run}.kraken2.unclassified"[23;5Hbenchmark:[K[24;9H"{PATH}/result/classification/benchmarks/{run}/{sample}_{run}.kraken2.benchmark.txt"[3d    benchmark:[K[4;9H"{PATH}/result/classification/benchmarks/{run}/{sample}_{run}.kraken2.benchmark.txt"[5;5Hthreads: 8[6;5Hparams:[K[7;9Hrunid=get_run[K[8;5Hlog:[K[9d'{PATH}/result/classification/kraken2/{run}/{sample}_{run}.kraken2.log'[K[10;5Hconda:[K[11d'envs/main.yaml'[K[12;5Hrun:[K[13d# --confidence          threshold that must be in [0,1][K[14;9H# --unclassified-out    prints unclassified sequences to filename[15;8H[1K # --classified-out[33Gprints classified sequences to filename[16;5H    # --output[16;33Hprints output to filename[17;9H# --report[14X[17;33Hprints report with aggregate counts/clade to file[18d[K[19;5H    if 'default' in {params.runid}:[20;9H    shell('kraken2 --db {input.db} --unclassified-out {output.unclassified} --report {output$[21;9Helif 'medium' in {params.runid}:#confidence set[K[22;9H    shell('kraken2 --db {input.db} --report {output.report} --confidence 0.05 --threads {thr$[23;5H    elif 'restrictive' in {params.runid}:[24;9H    print("Sure")[K[3d        elif 'restrictive' in {params.runid}:[4;9H    print("Sure")[K[5;5H    else: [6;12H[1K print("Kraken2 -- Nothing to do here:", {params.runid})[7d[K[8drule kaiju:[9;5Hinput:[K[10;5H    db = DI['kaiju']+"/kaiju_db_refseq.fmi",[11;9Hnodes = DI['kaiju']+"/nodes.dmp",[12;5H    files = "{PATH}/data/{sample}.fastq"[13;5Houtput:[K[14;9Hfiles = '{PATH}/result/classification/kaiju/{run}/{sample}_{run}.kaiju.classification'[15;5Hbenchmark:[K[16;9H'{PATH}/result/classification/benchmarks/{run}/{sample}_{run}.kaiju.benchmark.txt'[17;5Hthreads: 4[K[18;5Hparams:[19;9Hrunid=get_run,[K[20;9HmedianHitLength=get_medianHitLength[K[21;5Hlog:[K[22d'{PATH}/result/classification/kaiju/{run}/{sample}_{run}.kaiju.log'[K[23;5Hconda:[K[24d'envs/main.yaml' [3d    conda:[K[4d'envs/main.yaml' [5;5Hrun:[K[6d# -t    name of nodes.dmp file[K[7;9H# -f    name of database (.fmi) file[8;8H[1K # -i    input file containing fasta/fastq[9;5H    # -o    name of output file[10;9H# -m    minimum match length (default: 11)[11;9H# -E    minimum e-value in Greedy mode (which is default)[12;9Hif 'default' in {params.runid}:[K[13;12H[1K shell('kaiju -t {input.nodes} -f {input.db} -i {input.files} -o {output.files} -z {threa$[14;9Helif 'medianHitLength' in {params.runid}:[K[15;12H[1K shell('kaiju -t {input.nodes} -f {input.db} -i {input.files} -o {output.files} -z {threa$[16;9Helif 'restrictive' in {params.runid}:[K[17;12H[1K pass[18;5H    else:[19;9H    print("Kaiju -- Nothing to do here:", {params.runid})[20d[K[21drule kaiju_summary:[22;5Hinput:[K[23;5H    nodes = DI['kaiju']+"/nodes.dmp",[24;9Hnames= DI['kaiju']+"/names.dmp",[3d[2;15r[2;1H[5T[1;27r[3;9Hnodes = DI['kaiju']+"/nodes.dmp",[4;9Hnames= DI['kaiju']+"/names.dmp",[5;9Hfiles = "{PATH}/result/classification/kaiju/{run}/{sample}_{run}.kaiju.classification"[6;5Houtput:[7;9H"{PATH}/result/classification/kaiju/{run}/{sample}_{run}.kaiju.report"[10;5Hshell:[11d"kaiju2table -t {input.nodes} -n {input.names} -r species -o {output} {input.files}"[12d[K[13drule taxmaps: # many folders, fix output[K[14;5Hinput:[K[15ddb = DI['taxmaps']+"/refseq_complete_bacarchvir.lcak300.gem",[16;9Htaxonomy = DI['taxmaps']+"/taxonomy.tbl.gz",[17;9Hnodes = DI['kaiju']+"/nodes.dmp",[18;9Hfiles = "{PATH}/data/{sample}.fastq"[19;5Hbenchmark:[K[20;9H'{PATH}/result/classification/benchmarks/{run}/{sample}_{run}.taxmaps.benchmark.txt'[21d    output:[K[22;5H    o = '{PATH}/result/classification/taxmaps/{run}/{sample}_{run}.taxmaps.classification',[23;9Hfolder = '{PATH}/result/classification/taxmaps/{run}/{sample}'[24;5Hthreads: 8[K[3d[3;9Hfolder = '{PATH}/result/classification/taxmaps/{run}/{sample}'[4;5Hthreads: 8[K[5;5Hparams:[K[6;5H    runid=get_run,[7;9Hprefix = "{sample}_{run}.taxmaps.classification"[K[8;5Hlog:[K[9d"{PATH}/result/classification/taxmaps/{run}/{sample}_{run}.taxmaps.log"[10;5Hconda[11d'envs/taxmaps.yaml'[K[12;5Hrun:[13d[1K # -f        input fastq[K[14;5H    # -l[14;21Hin preprocessing: minimum read length for mapping[15;9H# -C        in preprocessing: entropy cutoff for low complexity filtering[16;9H# -d        index files[K[17;9H# -t        taxonomic rable[K[18;9H# --cov     coverage histogram[K[19;5H    # -o  [21Goutput directory[20;9Hif 'default' in {params.runid}:[K[21;12H[1K shell('export PERL5LIB=/home/re85gih/miniconda3/envs/taxmaps/opt/krona/lib/'),[22;9H    shell('export PATH=$PATH:/home/re85gih/projectClassification/taxmaps/'),[K[23;9H    shell('taxMaps -f {input.files} -c {threads} -t {input.taxonomy} -d {input.db} -o {outpu$[24;5H    elif 'medium' in {params.runid}:[3d[3;9H    shell('taxMaps -f {input.files} -c {threads} -t {input.taxonomy} -d {input.db} -o {outpu$[4;5H    elif 'medium' in {params.runid}:[5;12H[1K pass[6;9Helif 'restrictive' in {params.runid}:[7;9H    pass[K[8;5H    else:[9;9H    print("TaxMaps -- Nothing to do here:", {params.runid})[K[10d[K[11drule deepmicrobes:[K[12;5Hinput:[13dfiles = "{PATH}/data/{sample}.fastq",[14;9Hkmers =DI["deepmicrobes"]+"/tokens_merged_12mers.txt",[K[15;9Hweights=DI["deepmicrobes"]+"/weights_species",[K[16;9Hname2label=DI["deepmicrobes"]+"/name2label_species.txt"[17;5Houtput:[K[18;9Hprediction="{PATH}/result/classification/deepmicrobes/{run}/{sample}_{run}.deepmicrobes.pred$[19;9Htfrec="{PATH}/result/classification/deepmicrobes/{run}/{sample}_{run}.deepmicrobes.training.$[20;9Hclassification="{PATH}/result/classification/deepmicrobes/{run}/{sample}_{run}.deepmicrobes.$[21;9Hreport="{PATH}/result/classification/deepmicrobes/{run}/{sample}_{run}.deepmicrobes.report"[22;5Hconda:[K[23d'envs/deepmicrobes.yaml'[K[24;5Hbenchmark:[K[3d[3;9H'envs/deepmicrobes.yaml'[K[4;5Hbenchmark:[K[5;9H"{PATH}/result/classification/benchmarks/{run}/{sample}_{run}.deepmicrobes.benchmark.txt"[6;5Hthreads: 8[K[7;5Hrun:[K[8d# --kmer[8;25Hlength of k-mers (default: 12) --> if i want to change that i might need to $[9;9H# --max_len     max length of sequences (default: 150)[K[10;9H# --pred_out    path to prediction output[11;8H[1K # -dd     [25Glocation of input data[12;5H    # -ebe[12;25Hnumber of training epochs to run between evaluations[13;9H# just for me now[K[14d[K[15;9Hshell('export PATH=/home/re85gih/projectClassification/DeepMicrobes/pipelines:$PATH'),[16;9Hshell('export PATH=/home/re85gih/projectClassification/DeepMicrobes/scripts:$PATH'),[17;5H    shell('export PATH=/home/re85gih/projectClassification/DeepMicrobes:$PATH'),[18d[K[19;9H# transform training fastq to tfrec[K[20;9H#shell('tfrec_train_kmer.sh -i {input.files} -v {input.kmers} -o {output.tfrec}'),[K[21d[K[22;5H    # transform prediction fastq to tfrec[23;9Hshell('tfrec_predict_kmer.sh -f {input.files} -t fastq -v {input.kmers} -o {output.predictio$[24;1H[K[3d[3;9Hshell('tfrec_predict_kmer.sh -f {input.files} -t fastq -v {input.kmers} -o {output.predictio$[4;1H[K[5;9H# make prediction on metagenome datasaet[K[6;5H    shell('predict_DeepMicrobes.sh -i {output.prediction} -l species -p 8 -m {input.weights} -o $[7;1H[K[8;11Hgenerate taxonomic profiles[K[9;9Hshell('report_profile.sh -i {output.classification} -o {output.report} -t 50 -l {input.name2$[10;1H[K[11drule kslam:[K[12;5Hinput:[K[13ddb = DI['kslam'],[14;9Hfiles = "{PATH}/data/{sample}.fastq"[15;5Houtput:[K[16;9H"{PATH}/result/classification/kslam/{run}/{sample}_{run}.kslam.classification"[K[17;5Hbenchmark:[K[18;9H"{PATH}/result/classification/benchmarks/{run}/{sample}_{run}.kslam.benchmark.txt"[19;5Hthreads: 16[K[20;5Hparams:[K[21;9Hrunid=get_run[22;5Hlog:[K[23d"{PATH}/result/classification/kslam/{run}/{sample}_{run}.kslam.log"[K[24;5Hconda:[3d[3;9H"{PATH}/result/classification/kslam/{run}/{sample}_{run}.kslam.log"[K[4;5Hconda:[5d'envs/kslam.yaml'[K[6;5Hrun:[K[7d# --db[7;37Hdatabase file[8;11H--min-alignment-score     alignment score cutoff[9;9Hif 'default' in {params.runid}:[K[10;13Hshell('SLAM --db={input.db} --output-file={output} --num-reads-at-once 1000000 {input.fi$[11;8H[1K elif 'medium' in {params.runid}:[12;12H[1K pass[13;9Helif 'restrictive' in {params.runid}:[14;9H    pass[K[15;5H    else:[16;9H    print("KSLAM -- Nothing to do here:", {params.runid})[K[17d[K[18drule clark: #output is csv, watch out[K[19;5Hinput:[K[20;5H    #db = DI['clark']+"/",[21;9Hfiles = "{PATH}/data/{sample}.fastq",[22;5H    targets = "/mnt/fass1/database/clark_database/targets.txt"[23;5Houtput:[K[24;5H    helper = "{PATH}/result/classification/clark/{run}/{sample}_{run}.clark.classification.csv",[3d[4d[5d[6d[7d[8d[9d[10d[11d[12d[13d[14d[25d[K[15d[16d[17d[18d[19d[20d[21d[22d[23d[24d[3;9Helse:[K[4d[1K print("KSLAM -- Nothing to do here:", {params.runid})[5d[K[6drule clark: #output is csv, watch out[7;5Hinput:[K[8ddb = DI['clark']+"/",[K[9;9Hfiles = "{PATH}/data/{sample}.fastq",[10;9Htargets = "/mnt/fass1/database/clark_database/targets.txt"[K[11;5Houtput:[K[12;9Hhelper = "{PATH}/result/classification/clark/{run}/{sample}_{run}.clark.classification.csv",[13;9Hfiles= "{PATH}/result/classification/clark/{run}/{sample}_{run}.clark.classification"[14;5Hbenchmark:  [15;9H"{PATH}/result/classification/benchmarks/{run}/{sample}_{run}.clark.benchmark.txt"[16;5Hthreads: 8[K[17;5Hparams:[18d[1K db = "/mnt/fass1/kirsten/database/clark/",[19;12H[1K runid=get_run[20;5Hlog:[K[21d"{PATH}/result/classification/clark/{run}/{sample}_{run}.clark.log"[22;5Hconda:[K[23;5H    'envs/main.yaml'[24;5Hrun:[K[13d[14d[15d[16d[17d[18d[19d[20d[21d[22d[23d[24d[3;9H"{PATH}/result/classification/benchmarks/{run}/{sample}_{run}.clark.benchmark.txt"[4;5Hthreads: 8[K[5;5Hparams:[6d[1K db = "/mnt/fass1/kirsten/database/clark/",[7;12H[1K runid=get_run[8;5Hlog:[K[9d"{PATH}/result/classification/clark/{run}/{sample}_{run}.clark.log"[10;5Hconda:[K[11;5H    'envs/main.yaml'[12;5Hrun:[K[13d# -k        k-mer size, has to be between 2 and 32, default:31[K[14;5H    # --long    for long reads (only for full mode)[15;9H# -m        mode of execution[K[16d[K[17;5H    if 'default' in {params.runid}:[18;13Hshell('CLARK --long -O {input.files} -R {output.files} -D {input.db} -n {threads} -T {in$[19;9Helif 'medium' in {params.runid}:[20;12H[1K pass[21;9Helif 'restrictive' in {params.runid}:[K[22;12H[1K pass[23;9Helse:[K[24d[1K print("CLARK -- Nothing to do here:", {params.runid})[13d[14d[15d[16d[17d[18d[19d [A           shell('CLARK --long -O {input.files} -R {output.files} -D {input[1;92H(B[0;7mModified[18;76H(B[m.db} -n {threads} -T {inp[18;76H.db} -n {threads} -T {inpu[18;75H.db} -n {threads} -T {input[18;74H.db} -n {threads} -T {input.[18;73H.db} -n {threads} -T {input.t[18;72Hp.db} -n {threads} -T {input.[18;73Ha.db} -n {threads} -T {input[18;74Hr.db} -n {threads} -T {inpu[18;75Ha.db} -n {threads} -T {inp[18;76Hm.db} -n {threads} -T {in[18;77Hs.db} -n {threads} -T {i[18;78H[25d(B[0;7mSave modified buffer?  (Answering "No" will DISCARD changes.)                                        [26;1H Y(B[m Yes[K[27d(B[0;7m N(B[m No  [15G  (B[0;7m^C(B[m Cancel[K[25;63H[26d(B[0;7m^G(B[m Get Help[26;26H(B[0;7mM-D(B[m DOS Format[26;51H(B[0;7mM-A(B[m Append[26;76H(B[0;7mM-B(B[m Backup File[27d(B[0;7m^C(B[m Cancel[17G         (B[0;7mM-M(B[m Mac Format[27;51H(B[0;7mM-P(B[m Prepend[27;76H(B[0;7m^T(B[m To Files[25d(B[0;7mFile Name to Write: Snakefile                                (B[m[25;30H[?25l[25;41H[1K (B[0;7m[ Wrote 527 lines ](B[m[K[J[1;92H(B[0;7m        [27;101H(B[m[27;1H[?12l[?25h[?1049l[?1l>(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:08:10 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m
(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:08:10 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m
(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:08:10 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0mnano Snakefilee [A(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:08:10 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0msnakemake --use-conda -j 8
[33mProvided cores: 8[0m
[33mRules claiming more threads will be scaled down.[0m
[33mJob counts:
	count	jobs
	1	all
	1	clark
	2[0m
[32m[0m
[32mrule clark:
    input: /mnt/fass1/database/clark_database/targets.txt, /mnt/fass1/kirsten/data/gridion366.fastq
    output: /mnt/fass1/kirsten/result/classification/clark/default/gridion366_default.clark.classification.csv, /mnt/fass1/kirsten/result/classification/clark/default/gridion366_default.clark.classification
    log: /mnt/fass1/kirsten/result/classification/clark/default/gridion366_default.clark.log
    jobid: 1
    benchmark: /mnt/fass1/kirsten/result/classification/benchmarks/default/gridion366_default.clark.benchmark.txt
    wildcards: run=default, PATH=/mnt/fass1/kirsten, sample=gridion366
    threads: 8[0m
[32m[0m
Failed to find/read the directory:  /mnt/fass1/kirsten/database/clark/
[31mError in job clark while creating output files /mnt/fass1/kirsten/result/classification/clark/default/gridion366_default.clark.classification.csv, /mnt/fass1/kirsten/result/classification/clark/default/gridion366_default.clark.classification.[0m
[31mRuleException:
CalledProcessError in line 320 of /data/fass1/kirsten/result/classificationBenchmark/Snakefile:
Command 'CLARK --long -O /mnt/fass1/kirsten/data/gridion366.fastq -R /mnt/fass1/kirsten/result/classification/clark/default/gridion366_default.clark.classification -D /mnt/fass1/kirsten/database/clark/ -n 8 -T /mnt/fass1/database/clark_database/targets.txt' returned non-zero exit status 1
  File "/data/fass1/kirsten/result/classificationBenchmark/Snakefile", line 320, in __rule_clark
  File "/usr/lib/python3.5/concurrent/futures/thread.py", line 55, in run[0m
[31mExiting because a job execution failed. Look above for error message[0m
[33mWill exit after finishing currently running jobs.[0m
[31mExiting because a job execution failed. Look above for error message[0m
(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:08:28 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:08:28 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:08:28 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:08:28 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:08:28 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:08:28 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:08:28 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:08:28 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:08:28 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:08:28 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:08:28 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:08:28 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:08:28 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:08:28 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:08:28 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:08:28 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:08:28 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:08:28 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:08:28 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0mcd ../classification/clark/default/gridion3664_default.clark.
gridion364_default.clark.areport         gridion364_default.clark.report          
gridion364_default.clark.classification  gridion364_default.clark.stats           
(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:08:28 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0mcd ../classification/clark/default/gridion3664_default.clark.[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[A[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[K
[K[A[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K../databases/clark/db_central_k31_t156625_s1610612741_m0.tsk.
db_central_k31_t15625_s1610612741_m0.tsk.ky  db_central_k31_t15625_s1610612741_m0.tsk.sz
db_central_k31_t15625_s1610612741_m0.tsk.lb  
(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:08:28 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0mcd ../../databases/clark/db_central_k31_t156625_s1610612741_m0.tsk.[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[A[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[K
[K[A[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[Knano Snakefile 
[?1049h[1;27r(B[m[4l[?7h[?1h=[?1h=[?1h=[39;49m[?25l[39;49m(B[m[H[2J[25;58H(B[0;7m[ Reading File ](B[m[25;57H(B[0;7m[ Read 527 lines ](B[m[H[39;49m(B[0;7m  GNU nano 2.7.4                                             File: Snakefile                                                       [3;1H(B[m# Snakefile for project work of benchmarking different classification tools regarding their usability for long reads[5dconfigfile: "config.yaml"[7d########## VARIABLE DEFINITION[8dDI= dict(config["dataIndex"])[9dPATH = config["path"][10dSAMPLES = "gridion366" #list(config["samples"])[11dTOOLS= 'clark'#ccmetagen centrifuge kraken2 clark kaiju'.split(" ") #list(config["classification"])[12dRUNS='default'#'medianHitLength'#'quals'# 'default medium restrictive'.split()[14dimport scripts.getting[17drule all:[18;5Hinput:[19d# CLASSIFICATION[20;9Hexpand("{path}/result/classification/{tool}/{run}/{sample}_{run}.{tool}.classification", run=RUNS, sample=SAMPLES, tool=TO$[21;1H# GENERATING (comparable) REPORTS[22d#[9Gexpand("{path}/result/classification/{tool}/{run}/{sample}_{run}.{tool}.areport", run=RUNS, sample=SAMPLES, tool=TOOLS, pa$[23;1H#[23;10Hexpand("{path}/result/classification/{tool}/{run}/{sample}_{run}.{tool}.report", run=RUNS, sample=SAMPLES, tool=TOOLS, pa$[24;1H#[9Gexpand("{path}/result/classification/ccmetagen/{sample}.{tool}.intermediate.res", run=RUNS, sample=SAMPLES, tool=TOOLS, pa$[26;1H(B[0;7m^G(B[m Get Help     (B[0;7m^O(B[m Write Out    (B[0;7m^W(B[m Where Is     (B[0;7m^K(B[m Cut Text     (B[0;7m^J(B[m Justify[81G(B[0;7m^C(B[m Cur Pos[97G(B[0;7m^Y(B[m Prev Page    (B[0;7mM-\(B[m First Line[27d(B[0;7m^X(B[m Exit[27;17H(B[0;7m^R(B[m Read File    (B[0;7m^\(B[m Replace[49G(B[0;7m^U(B[m Uncut Text   (B[0;7m^T(B[m To Spell     (B[0;7m^_(B[m Go To Line   (B[0;7m^V(B[m Next Page    (B[0;7mM-/(B[m Last Line[25d[?12l[?25h[3d#        expand("{path}/result/classification/{tool}/{run}/{sample}_{run}.{tool}.report", run=RUNS, sample=SAMPLES, tool=TOOLS, pa$[4;1H#[9Gexpand("{path}/result/classification/ccmetagen/{sample}.{tool}.intermediate.res", run=RUNS, sample=SAMPLES, tool=TOOLS, pa$[5;1H#       expand("{path}/result/classification/{tool}/default/{sample}_{run}.catbat.bins",tool=TOOLS, run=RUNS, sample=SAMPLES, path$[7;1Hrule diamond_db:[K[8d    input:[K[9;8H[1K faa="/mnt/fass1/kirsten/databases/diamond_all/faa/full_proteome_bacteria_scerevisiae_cneoformans.faa",[10;8H[1K map="/mnt/fass1/genomes/new_bacteria/bacteria_blast_db/prot_accession2taxid.txt",[11;8H[1K nodes="/mnt/fass1/kirsten/databases/diamond/nodes.dmp",[K[12;8H[1K names="/mnt/fass1/kirsten/databases/diamond/names.dmp"[K[13;5Houtput:[14;8H[1K "/mnt/fass1/kirsten/databases/diamond_all/nr.dmnd"[15;5Hbenchmark:[16;9H"/mnt/fass1/kirsten/result/classification/benchmarks/diamond.db.benchmark.txt"[17d    conda:[18;5H    "envs/diamond.yaml"[19d    params:[K[20;9H"/mnt/fass1/kirsten/databases/diamond_all/nr"[K[21d    threads: 8[K[22d    shell:[K[23;8H[1K "diamond makedb --in {input.faa} -d {params} --taxonmap {input.map} --taxonnodes {input.nodes} --taxonnames {input.names}" [24;1H[K[3d[3;8r[8;1H[2S[1;27r[3;8H[1K "diamond makedb --in {input.faa} -d {params} --taxonmap {input.map} --taxonnodes {input.nodes} --taxonnames {input.names}" [5;6Hcentrifuge_db:[7;9Hmap = "/mnt/fass1/kirsten/databases/centrifuge_all/seqid2taxid.map",[8;9Hnodes = "/mnt/fass1/kirsten/databases/centrifuge_all/taxonomy/nodes.dmp",[9;9Hnames ="/mnt/fass1/kirsten/databases/centrifuge_all/taxonomy/names.dmp",[K[10;9Hfaa[10;25Hkirsten/databases/centrifuge_all/fna/full_bacteria_scerevisiae_cneoformans.fna"[11;5Houtput:[K[12;8Hfile1= [12;45Hcentrifuge_all/bac_cer_neo.1.cf",[13;5H   file2= "/mnt/fass1/kirsten/databases/centrifuge_all/bac_cer_neo.2.cf",[14;8Hfile3= "/mnt/fass1/kirsten/databases/centrifuge_all/bac_cer_neo.3.cf"[15;5Hthreads: 8[16;5Hbenchmark:[K[17;5H    "/mnt/fass1/kirsten/result/classification/benchmarks/centrifuge.db.benchmark.txt"[18;5Hparams:[K[19;5H    "/mnt/fass1/kirsten/databases/centrifuge_all/bar_cer_neo"[20;5Hconda:[K[21;5H    "envs/centrifuge.yaml"[23;10Hcentrifuge-build -p {threads} --conversion-table {input.map} --taxonomy-tree {input.nodes} --name-table {input.names} {in$[3;1H[3;10Hcentrifuge-build -p {threads} --conversion-table {input.map} --taxonomy-tree {input.nodes} --name-table {input.names} {in$[5;1H# creating the project structure[6drule create:[7;5Hshell:[K[8d'python structure.py'[K[9d[K[10ddef get_run(wildcards): #returns the current value of variable/wildcard run[K[11;5Hreturn wildcards.run[12d[K[13ddef get_tool(wildcards): #returns the current value of variable/wildcard run [14;5Hreturn wildcards.tool[K[15d[K[16ddef get_medianHitLength(wildcards):[17;5Hreturn scripts.getting.get_seqLength(str(PATH[0])+"/data/"+str(wildcards.sample)+".fastq")#/2[18d #   elif str(wildcards.tool) == "kaiju":[19;3H#      lengths_default = {'gridion364':201, 'gridion366':194}  [20;4H#     return lengths_default[str(wildcards.sample)]/2[21;5H# return wildcards.sample # scripts.getting.get_seqLength({PATH}+str(wildcards.sample)+".fastq")/2[22d[K[23drule centrifuge:[K[24;5Hinput:[3drule centrifuge:[K[4;5Hinput:[5;8H[1K fastq = "{PATH}/data/{sample}.fastq"[6d    output:[K[7;5H    files = "{PATH}/result/classification/centrifuge/{run}/{sample}_{run}.centrifuge.classification",[8;9Hreport= "{PATH}/result/classification/centrifuge/{run}/{sample}_{run}.centrifuge.report"[9;5Hbenchmark:[10;8H[1K "{PATH}/result/classification/benchmarks/{run}/{sample}_{run}.centrifuge.benchmark.txt"[11;5Hthreads: 8[K[12;5Hparams:[13;8H[1K runid=get_run,[K[14;5H    db = DI["centrifuge"],[15d#[15;10Hmedianlength=get_medianHitLength[16d    conda:[K[17;5H   "envs/centrifuge.yaml"[K[18d    run:[K[19d[1K # -q[41m                            [49m(B[mfiles are fastq[K[20d[41m        [49m(B[m# -x[41m                            [49m(B[mindex files[K[21d[41m        [49m(B[m# --report-file[41m         [49m(B[mgenerated report file[K[22d[41m        [49m(B[m# -S[41m                            [49m(B[moutput file[23;8H[1K # -f    [23;29Hquery input files are (multi)fasta[24;5H    # --ignore-quals[3d[8G[1K # -f    [3;29Hquery input files are (multi)fasta[4;5H    # --ignore-quals[5d[K[6;5H    if 'default' in {params.runid}:[7;9H    shell('centrifuge -q -x {params.db} {input.fastq} --report-file {output.report} -S {output.files} --ignore-quals')[8;9Helif 'quals' in {params.runid}:[K[9;12H[1K shell('centrifuge -q -x {params.db} {input.fastq} --report-file {output.report} -S {output.files}')[10;9Helif 'medianHitLength' in {params.runid}:[K[11;13H[1K shell('centrifuge -q -x {params.db} {input.fastq} --report-file {output.report} -S {output.files} --ignore-quals --mi$[12;5H    else:[13;9H    print("Centrifuge -- Nothing to be done here:", {params.runid})[14d[K[15drule kraken2:[K[16;5Hinput[17d db = DI['kraken2'],  [18;5H    files = "{PATH}/data/{sample}.fastq"[19;5Houtput:[K[20;8H[1K files = "{PATH}/result/classification/kraken2/{run}/{sample}_{run}.kraken2.classification",[21;8H[1K report= "{PATH}/result/classification/kraken2/{run}/{sample}_{run}.kraken2.report",[22;8H[1K unclassified="{PATH}/result/classification/kraken2/{run}/{sample}_{run}.kraken2.unclassified"[23;5Hbenchmark:[K[24;9H"{PATH}/result/classification/benchmarks/{run}/{sample}_{run}.kraken2.benchmark.txt"[3d    benchmark:[K[4;9H"{PATH}/result/classification/benchmarks/{run}/{sample}_{run}.kraken2.benchmark.txt"[5;5Hthreads: 8[6;5Hparams:[K[7;9Hrunid=get_run[K[8;5Hlog:[K[9d'{PATH}/result/classification/kraken2/{run}/{sample}_{run}.kraken2.log'[K[10;5Hconda:[K[11d'envs/main.yaml'[K[12;5Hrun:[K[13d# --confidence          threshold that must be in [0,1][K[14;9H# --unclassified-out    prints unclassified sequences to filename[15;8H[1K # --classified-out[33Gprints classified sequences to filename[16;5H    # --output[16;33Hprints output to filename[17;9H# --report[14X[17;33Hprints report with aggregate counts/clade to file[18d[K[19;5H    if 'default' in {params.runid}:[20;9H    shell('kraken2 --db {input.db} --unclassified-out {output.unclassified} --report {output.report} --threads {threads} -$[21;9Helif 'medium' in {params.runid}:#confidence set[K[22;9H    shell('kraken2 --db {input.db} --report {output.report} --confidence 0.05 --threads {threads} --output {output.files} $[23;5H    elif 'restrictive' in {params.runid}:[24;9H    print("Sure")[K[3d        elif 'restrictive' in {params.runid}:[4;9H    print("Sure")[K[5;5H    else: [6;12H[1K print("Kraken2 -- Nothing to do here:", {params.runid})[7d[K[8drule kaiju:[9;5Hinput:[K[10;5H    db = DI['kaiju']+"/kaiju_db_refseq.fmi",[11;9Hnodes = DI['kaiju']+"/nodes.dmp",[12;5H    files = "{PATH}/data/{sample}.fastq"[13;5Houtput:[K[14;9Hfiles = '{PATH}/result/classification/kaiju/{run}/{sample}_{run}.kaiju.classification'[15;5Hbenchmark:[K[16;9H'{PATH}/result/classification/benchmarks/{run}/{sample}_{run}.kaiju.benchmark.txt'[17;5Hthreads: 4[K[18;5Hparams:[19;9Hrunid=get_run,[K[20;9HmedianHitLength=get_medianHitLength[K[21;5Hlog:[K[22d'{PATH}/result/classification/kaiju/{run}/{sample}_{run}.kaiju.log'[K[23;5Hconda:[K[24d'envs/main.yaml' [3d    conda:[K[4d'envs/main.yaml' [5;5Hrun:[K[6d# -t    name of nodes.dmp file[K[7;9H# -f    name of database (.fmi) file[8;8H[1K # -i    input file containing fasta/fastq[9;5H    # -o    name of output file[10;9H# -m    minimum match length (default: 11)[11;9H# -E    minimum e-value in Greedy mode (which is default)[12;9Hif 'default' in {params.runid}:[K[13;12H[1K shell('kaiju -t {input.nodes} -f {input.db} -i {input.files} -o {output.files} -z {threads} -v')[14;9Helif 'medianHitLength' in {params.runid}:[K[15;12H[1K shell('kaiju -t {input.nodes} -f {input.db} -i {input.files} -o {output.files} -z {threads} -v -m {params.medianHitLen$[16;9Helif 'restrictive' in {params.runid}:[K[17;12H[1K pass[18;5H    else:[19;9H    print("Kaiju -- Nothing to do here:", {params.runid})[20d[K[21drule kaiju_summary:[22;5Hinput:[K[23;5H    nodes = DI['kaiju']+"/nodes.dmp",[24;9Hnames= DI['kaiju']+"/names.dmp",[3d[2;15r[2;1H[5T[1;27r[3;9Hnodes = DI['kaiju']+"/nodes.dmp",[4;9Hnames= DI['kaiju']+"/names.dmp",[5;9Hfiles = "{PATH}/result/classification/kaiju/{run}/{sample}_{run}.kaiju.classification"[6;5Houtput:[7;9H"{PATH}/result/classification/kaiju/{run}/{sample}_{run}.kaiju.report"[10;5Hshell:[11d"kaiju2table -t {input.nodes} -n {input.names} -r species -o {output} {input.files}"[12d[K[13drule taxmaps: # many folders, fix output[K[14;5Hinput:[K[15ddb = DI['taxmaps']+"/refseq_complete_bacarchvir.lcak300.gem",[16;9Htaxonomy = DI['taxmaps']+"/taxonomy.tbl.gz",[17;9Hnodes = DI['kaiju']+"/nodes.dmp",[18;9Hfiles = "{PATH}/data/{sample}.fastq"[19;5Hbenchmark:[K[20;9H'{PATH}/result/classification/benchmarks/{run}/{sample}_{run}.taxmaps.benchmark.txt'[21d    output:[K[22;5H    o = '{PATH}/result/classification/taxmaps/{run}/{sample}_{run}.taxmaps.classification',[23;9Hfolder = '{PATH}/result/classification/taxmaps/{run}/{sample}'[24;5Hthreads: 8[K[3d[3;9Hfolder = '{PATH}/result/classification/taxmaps/{run}/{sample}'[4;5Hthreads: 8[K[5;5Hparams:[K[6;5H    runid=get_run,[7;9Hprefix = "{sample}_{run}.taxmaps.classification"[K[8;5Hlog:[K[9d"{PATH}/result/classification/taxmaps/{run}/{sample}_{run}.taxmaps.log"[10;5Hconda[11d'envs/taxmaps.yaml'[K[12;5Hrun:[13d[1K # -f        input fastq[K[14;5H    # -l[14;21Hin preprocessing: minimum read length for mapping[15;9H# -C        in preprocessing: entropy cutoff for low complexity filtering[16;9H# -d        index files[K[17;9H# -t        taxonomic rable[K[18;9H# --cov     coverage histogram[K[19;5H    # -o  [21Goutput directory[20;9Hif 'default' in {params.runid}:[K[21;12H[1K shell('export PERL5LIB=/home/re85gih/miniconda3/envs/taxmaps/opt/krona/lib/'),[22;9H    shell('export PATH=$PATH:/home/re85gih/projectClassification/taxmaps/'),[K[23;9H    shell('taxMaps -f {input.files} -c {threads} -t {input.taxonomy} -d {input.db} -o {output.folder} -p {params.prefix}')[24;5H    elif 'medium' in {params.runid}:[3d[3;9H    shell('taxMaps -f {input.files} -c {threads} -t {input.taxonomy} -d {input.db} -o {output.folder} -p {params.prefix}')[4;5H    elif 'medium' in {params.runid}:[5;12H[1K pass[6;9Helif 'restrictive' in {params.runid}:[7;9H    pass[K[8;5H    else:[9;9H    print("TaxMaps -- Nothing to do here:", {params.runid})[K[10d[K[11drule deepmicrobes:[K[12;5Hinput:[13dfiles = "{PATH}/data/{sample}.fastq",[14;9Hkmers =DI["deepmicrobes"]+"/tokens_merged_12mers.txt",[K[15;9Hweights=DI["deepmicrobes"]+"/weights_species",[K[16;9Hname2label=DI["deepmicrobes"]+"/name2label_species.txt"[17;5Houtput:[K[18;9Hprediction="{PATH}/result/classification/deepmicrobes/{run}/{sample}_{run}.deepmicrobes.prediction.tfrec",[19;9Htfrec="{PATH}/result/classification/deepmicrobes/{run}/{sample}_{run}.deepmicrobes.training.tfrec",[20;9Hclassification="{PATH}/result/classification/deepmicrobes/{run}/{sample}_{run}.deepmicrobes.classification",[21;9Hreport="{PATH}/result/classification/deepmicrobes/{run}/{sample}_{run}.deepmicrobes.report"[22;5Hconda:[K[23d'envs/deepmicrobes.yaml'[K[24;5Hbenchmark:[K[3d[3;9H'envs/deepmicrobes.yaml'[K[4;5Hbenchmark:[K[5;9H"{PATH}/result/classification/benchmarks/{run}/{sample}_{run}.deepmicrobes.benchmark.txt"[6;5Hthreads: 8[K[7;5Hrun:[K[8d# --kmer[8;25Hlength of k-mers (default: 12) --> if i want to change that i might need to build my own index[9;9H# --max_len     max length of sequences (default: 150)[K[10;9H# --pred_out    path to prediction output[11;8H[1K # -dd     [25Glocation of input data[12;5H    # -ebe[12;25Hnumber of training epochs to run between evaluations[13;9H# just for me now[K[14d[K[15;9Hshell('export PATH=/home/re85gih/projectClassification/DeepMicrobes/pipelines:$PATH'),[16;9Hshell('export PATH=/home/re85gih/projectClassification/DeepMicrobes/scripts:$PATH'),[17;5H    shell('export PATH=/home/re85gih/projectClassification/DeepMicrobes:$PATH'),[18d[K[19;9H# transform training fastq to tfrec[K[20;9H#shell('tfrec_train_kmer.sh -i {input.files} -v {input.kmers} -o {output.tfrec}'),[K[21d[K[22;5H    # transform prediction fastq to tfrec[23;9Hshell('tfrec_predict_kmer.sh -f {input.files} -t fastq -v {input.kmers} -o {output.prediction}'),[24d[K[3d[3;9Hshell('tfrec_predict_kmer.sh -f {input.files} -t fastq -v {input.kmers} -o {output.prediction}'),[4d[K[5;9H# make prediction on metagenome datasaet[K[6;5H    shell('predict_DeepMicrobes.sh -i {output.prediction} -l species -p 8 -m {input.weights} -o {output.classification}'),[7d[K[8;11Hgenerate taxonomic profiles[K[9;9Hshell('report_profile.sh -i {output.classification} -o {output.report} -t 50 -l {input.name2label}')[10d[K[11drule kslam:[K[12;5Hinput:[K[13ddb = DI['kslam'],[14;9Hfiles = "{PATH}/data/{sample}.fastq"[15;5Houtput:[K[16;9H"{PATH}/result/classification/kslam/{run}/{sample}_{run}.kslam.classification"[K[17;5Hbenchmark:[K[18;9H"{PATH}/result/classification/benchmarks/{run}/{sample}_{run}.kslam.benchmark.txt"[19;5Hthreads: 16[K[20;5Hparams:[K[21;9Hrunid=get_run[22;5Hlog:[K[23d"{PATH}/result/classification/kslam/{run}/{sample}_{run}.kslam.log"[K[24;5Hconda:[3d[3;9H"{PATH}/result/classification/kslam/{run}/{sample}_{run}.kslam.log"[K[4;5Hconda:[5d'envs/kslam.yaml'[K[6;5Hrun:[K[7d# --db[7;37Hdatabase file[8;11H--min-alignment-score     alignment score cutoff[9;9Hif 'default' in {params.runid}:[K[10;13Hshell('SLAM --db={input.db} --output-file={output} --num-reads-at-once 1000000 {input.files}')[11;8H[1K elif 'medium' in {params.runid}:[12;12H[1K pass[13;9Helif 'restrictive' in {params.runid}:[14;9H    pass[K[15;5H    else:[16;9H    print("KSLAM -- Nothing to do here:", {params.runid})[K[17d[K[18drule clark: #output is csv, watch out[K[19;5Hinput:[K[20;5H    #db = DI['clark']+"/",[21;9Hfiles = "{PATH}/data/{sample}.fastq",[22;5H    targets = "/mnt/fass1/database/clark_database/targets.txt"[23;5Houtput:[K[24;5H    helper = "{PATH}/result/classification/clark/{run}/{sample}_{run}.clark.classification.csv",[3d[4d[5d[6d[7d[8d[9d[10d[11d[12d[13d[14d[25d[K[15d[16d[17d[18d[19d[20d        #[1;122H(B[0;7mModified[20;9H(B[m[1P[21d[22d[23d[24d[3;9Helse:[K[4d[1K print("KSLAM -- Nothing to do here:", {params.runid})[5d[K[6drule clark: #output is csv, watch out[7;5Hinput:[K[8ddb = DI['clark']+"/",[K[9;9Hfiles = "{PATH}/data/{sample}.fastq",[10;9Htargets = "/mnt/fass1/database/clark_database/targets.txt"[K[11;5Houtput:[K[12;9Hhelper = "{PATH}/result/classification/clark/{run}/{sample}_{run}.clark.classification.csv",[13;9Hfiles= "{PATH}/result/classification/clark/{run}/{sample}_{run}.clark.classification"[14;5Hbenchmark:  [15;9H"{PATH}/result/classification/benchmarks/{run}/{sample}_{run}.clark.benchmark.txt"[16;5Hthreads: 8[K[17;5Hparams:[18d[1K db = "/mnt/fass1/kirsten/database/clark/",[19;12H[1K runid=get_run[20;5Hlog:[K[21d"{PATH}/result/classification/clark/{run}/{sample}_{run}.clark.log"[22;5Hconda:[K[23;5H    'envs/main.yaml'[24;5Hrun:[K[13;9H[14d[15d[16d[17d[18d[19d[20d[21d[22d[23d[24d[3;9H"{PATH}/result/classification/benchmarks/{run}/{sample}_{run}.clark.benchmark.txt"[4;5Hthreads: 8[K[5;5Hparams:[6d[1K db = "/mnt/fass1/kirsten/database/clark/",[7;12H[1K runid=get_run[8;5Hlog:[K[9d"{PATH}/result/classification/clark/{run}/{sample}_{run}.clark.log"[10;5Hconda:[K[11;5H    'envs/main.yaml'[12;5Hrun:[K[13d# -k        k-mer size, has to be between 2 and 32, default:31[K[14;5H    # --long    for long reads (only for full mode)[15;9H# -m        mode of execution[K[16d[K[17;5H    if 'default' in {params.runid}:[18;13Hshell('CLARK --long -O {input.files} -R {output.files} -D {params.db} -n {threads} -T {input.targets}')[19;9Helif 'medium' in {params.runid}:[20;12H[1K pass[21;9Helif 'restrictive' in {params.runid}:[K[22;12H[1K pass[23;9Helse:[K[24d[1K print("CLARK -- Nothing to do here:", {params.runid})[13;9H[14d[15d[16d[17;9Hi[18d   shell('CLARK --long -O {input.files} -R {output.files} -D {params[1P[1P[1P[1P[1P[1Pi.db} -n {threads} -T {input.targets}')[18;73Hn.db} -n {threads} -T {input.targets}')[18;74Hp.db} -n {threads} -T {input.targets}')[18;75Hu.db} -n {threads} -T {input.targets}')[18;76Ht.db} -n {threads} -T {input.targets}')[18;77H[25d(B[0;7mSave modified buffer?  (Answering "No" will DISCARD changes.)                                                                      [26;1H Y(B[m Yes[K[27d(B[0;7m N(B[m No  [27;18H(B[0;7mC(B[m Cancel[K[25;63H[26d(B[0;7m^G(B[m Get Help[26;33H(B[0;7mM-D(B[m DOS Format[26;65H(B[0;7mM-A(B[m Append[26;97H(B[0;7mM-B(B[m Backup File[27d(B[0;7m^C(B[m Cancel[17G         [33G(B[0;7mM-M(B[m Mac Format[27;65H(B[0;7mM-P(B[m Prepend[27;97H(B[0;7m^T(B[m To Files[25d(B[0;7mFile Name to Write: Snakefile                                (B[m[25;30H[?25l[25;56H[1K (B[0;7m[ Wrote 527 lines ](B[m[K[J[1;122H(B[0;7m        [27;131H(B[m[27;1H[?12l[?25h[?1049l[?1l>(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:11:07 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0mnano Snakefile snakemake --use-conda -j 8
[31mMissingInputException in line 296 of /data/fass1/kirsten/result/classificationBenchmark/Snakefile:
Missing input files for rule clark:
/mnt/fass1/kirsten/database/clark/[0m
(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:11:09 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0mnano Snakefile 
[?1049h[1;27r(B[m[4l[?7h[?1h=[?1h=[?1h=[39;49m[?25l[39;49m(B[m[H[2J[25;58H(B[0;7m[ Reading File ](B[m[25;57H(B[0;7m[ Read 527 lines ](B[m[H[39;49m(B[0;7m  GNU nano 2.7.4                                             File: Snakefile                                                       [3;1H(B[m# Snakefile for project work of benchmarking different classification tools regarding their usability for long reads[5dconfigfile: "config.yaml"[7d########## VARIABLE DEFINITION[8dDI= dict(config["dataIndex"])[9dPATH = config["path"][10dSAMPLES = "gridion366" #list(config["samples"])[11dTOOLS= 'clark'#ccmetagen centrifuge kraken2 clark kaiju'.split(" ") #list(config["classification"])[12dRUNS='default'#'medianHitLength'#'quals'# 'default medium restrictive'.split()[14dimport scripts.getting[17drule all:[18;5Hinput:[19d# CLASSIFICATION[20;9Hexpand("{path}/result/classification/{tool}/{run}/{sample}_{run}.{tool}.classification", run=RUNS, sample=SAMPLES, tool=TO$[21;1H# GENERATING (comparable) REPORTS[22d#[9Gexpand("{path}/result/classification/{tool}/{run}/{sample}_{run}.{tool}.areport", run=RUNS, sample=SAMPLES, tool=TOOLS, pa$[23;1H#[23;10Hexpand("{path}/result/classification/{tool}/{run}/{sample}_{run}.{tool}.report", run=RUNS, sample=SAMPLES, tool=TOOLS, pa$[24;1H#[9Gexpand("{path}/result/classification/ccmetagen/{sample}.{tool}.intermediate.res", run=RUNS, sample=SAMPLES, tool=TOOLS, pa$[26;1H(B[0;7m^G(B[m Get Help     (B[0;7m^O(B[m Write Out    (B[0;7m^W(B[m Where Is     (B[0;7m^K(B[m Cut Text     (B[0;7m^J(B[m Justify[81G(B[0;7m^C(B[m Cur Pos[97G(B[0;7m^Y(B[m Prev Page    (B[0;7mM-\(B[m First Line[27d(B[0;7m^X(B[m Exit[27;17H(B[0;7m^R(B[m Read File    (B[0;7m^\(B[m Replace[49G(B[0;7m^U(B[m Uncut Text   (B[0;7m^T(B[m To Spell     (B[0;7m^_(B[m Go To Line   (B[0;7m^V(B[m Next Page    (B[0;7mM-/(B[m Last Line[25d[?12l[?25h[3d[25d[J[27;131H[27;1H[?1049l[?1l>(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:11:24 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0mnano Snakefile snakemake --use-conda -j 8[11Pnano Snakefile snakemake --use-conda -j 8[11Pnano Snakefile snakemake --use-conda -j 8cd result/classificationBenchmark/[8Ponda activate projectMAINexit[Kconda activate projectMAINd result/classificationBenchmark/[8Psnakemake --use-conda -j 8[11Pnano Snakefile snakemake --use-conda -j 8[11Pnano Snakefile snakemake --use-conda -j 8[11Pnano Snakefile [Knano Snakefile snakemake --use-conda -j 8[11Pnano Snakefile snakemake --use-conda -j 8[11Pnano Snakefile [Knano Snakefile snakemake --use-conda -j 8[11Pnano Snakefile snakemake --use-conda -j 8[11Pnano Snakefile [Knano Snakefile snakemake --use-conda -j 8[11Pnano Snakefile snakemake --use-conda -j 8[11Pnano Snakefile [K[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:11:24 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:11:24 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:11:24 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:11:24 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:11:24 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:11:24 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:11:24 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:11:24 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:11:24 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:11:24 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:11:24 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:11:24 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:11:24 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:11:24 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:11:24 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:11:24 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:11:24 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:11:24 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:11:24 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:11:24 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:11:24 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:11:24 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:11:24 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0mnano config.yaml 
[?1049h[1;36r(B[m[4l[?7h[?1h=[?1h=[?1h=[39;49m[?25l[39;49m(B[m[H[2J[34;58H(B[0;7m[ Reading File ](B[m[34;64H(B[0;7m 14 lines ](B[m[H[39;49m(B[0;7m  GNU nano 2.7.4                                            File: config.yaml                                                      [3;1H(B[m# path to folder where generated ourtputs should be saved, aka project folder[4dpath: [/mnt/fass1/kirsten][5d# path to the folders where the needed databases and/or indices are located[6ddataIndex: {taxmaps: '/mnt/fass1/kirsten/databases/taxmaps', deepmicrobes: '/mnt/fass1/kirsten/databases/deepMicrobes', kaiju: '/m$[8;1H# incorporated tools[10d# preprocessing: {} ???[12dclassification: {kaiju, diamond, taxmaps, deepmicrobes, kslam, kraken2, centrifuge, clark, metaothello, benchmarks, catbat}[14dsamples: {gridion364: '/mnt/fass1/kirsten/data/gridion364.fastq.gz', gridion366: '/mnt/fass1/kirsten/data/gridion366.fastq.gz', pr$[16;1Hspecies: ['Bacillus subtilis', 'Listeria monocytogenes', 'Enterococcus faecalis', 'Staphylococcus aureus', 'Salmonella enterica', $[35;1H(B[0;7m^G(B[m Get Help     (B[0;7m^O(B[m Write Out    (B[0;7m^W(B[m Where Is     (B[0;7m^K(B[m Cut Text     (B[0;7m^J(B[m Justify[81G(B[0;7m^C(B[m Cur Pos[97G(B[0;7m^Y(B[m Prev Page    (B[0;7mM-\(B[m First Line[36d(B[0;7m^X(B[m Exit[36;17H(B[0;7m^R(B[m Read File    (B[0;7m^\(B[m Replace[49G(B[0;7m^U(B[m Uncut Text   (B[0;7m^T(B[m To Spell     (B[0;7m^_(B[m Go To Line   (B[0;7m^V(B[m Next Page    (B[0;7mM-/(B[m Last Line[34d[?12l[?25h[3d[4d[5d[6ddataIndex: {taxmaps: '[34d[K[6;24Hmnt/fass1/kirsten/databases/taxmaps', deepmicrobes: '/mnt/fass1/kirsten/databases/deepMicrobes', kaiju: '/$u: '/mnt/fass1/kirsten/databases/kaiju', kaiju_all: '/mnt/fass1/kirsten/databases/kaiju_all',kslam: '/mnt/fass1/kirsten/databases[8Gnt/fass1/kirsten/databases/kaiju', kaiju_all: '/mnt/fass1/kirsten/databases/kaiju_all',kslam: '/mnt/fass1/kirsten/database$abases/kslam', kraken2: '/mnt/fass1/database/kraken2-database', centrifuge: '/mnt/fass1/kirsten/databases/centrifuge/p_compressed[8G/kslam', kraken2: '/mnt/fass1/database/kraken2-database', centrifuge: '/mnt/fass1/kirsten/databases/centrifuge/p_compres[35d[J[36;131H[36;1H[?1049l[?1l>(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:12:12 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0mnano Snakefile 
[?1049h[1;36r(B[m[4l[?7h[?1h=[?1h=[?1h=[39;49m[?25l[39;49m(B[m[H[2J[34;58H(B[0;7m[ Reading File ](B[m[34;57H(B[0;7m[ Read 527 lines ](B[m[H[39;49m(B[0;7m  GNU nano 2.7.4                                             File: Snakefile                                                       [3;1H(B[m# Snakefile for project work of benchmarking different classification tools regarding their usability for long reads[5dconfigfile: "config.yaml"[7d########## VARIABLE DEFINITION[8dDI= dict(config["dataIndex"])[9dPATH = config["path"][10dSAMPLES = "gridion366" #list(config["samples"])[11dTOOLS= 'clark'#ccmetagen centrifuge kraken2 clark kaiju'.split(" ") #list(config["classification"])[12dRUNS='default'#'medianHitLength'#'quals'# 'default medium restrictive'.split()[14dimport scripts.getting[17drule all:[18;5Hinput:[19d# CLASSIFICATION[20;9Hexpand("{path}/result/classification/{tool}/{run}/{sample}_{run}.{tool}.classification", run=RUNS, sample=SAMPLES, tool=TO$[21;1H# GENERATING (comparable) REPORTS[22d#[9Gexpand("{path}/result/classification/{tool}/{run}/{sample}_{run}.{tool}.areport", run=RUNS, sample=SAMPLES, tool=TOOLS, pa$[23;1H#[23;10Hexpand("{path}/result/classification/{tool}/{run}/{sample}_{run}.{tool}.report", run=RUNS, sample=SAMPLES, tool=TOOLS, pa$[24;1H#[9Gexpand("{path}/result/classification/ccmetagen/{sample}.{tool}.intermediate.res", run=RUNS, sample=SAMPLES, tool=TOOLS, pa$[25;1H#[9Gexpand("{path}/result/classification/{tool}/default/{sample}_{run}.catbat.bins",tool=TOOLS, run=RUNS, sample=SAMPLES, path$[27;1Hrule diamond_db:[28;5Hinput:[29dfaa="/mnt/fass1/kirsten/databases/diamond_all/faa/full_proteome_bacteria_scerevisiae_cneoformans.faa",[30;9Hmap="/mnt/fass1/genomes/new_bacteria/bacteria_blast_db/prot_accession2taxid.txt",[31;9Hnodes="/mnt/fass1/kirsten/databases/diamond/nodes.dmp",[32;9Hnames="/mnt/fass1/kirsten/databases/diamond/names.dmp"[33;5Houtput:[35d(B[0;7m^G(B[m Get Help     (B[0;7m^O(B[m Write Out    (B[0;7m^W(B[m Where Is     (B[0;7m^K(B[m Cut Text     (B[0;7m^J(B[m Justify[81G(B[0;7m^C(B[m Cur Pos[97G(B[0;7m^Y(B[m Prev Page    (B[0;7mM-\(B[m First Line[36d(B[0;7m^X(B[m Exit[36;17H(B[0;7m^R(B[m Read File    (B[0;7m^\(B[m Replace[49G(B[0;7m^U(B[m Uncut Text   (B[0;7m^T(B[m To Spell     (B[0;7m^_(B[m Go To Line   (B[0;7m^V(B[m Next Page    (B[0;7mM-/(B[m Last Line[34d[?12l[?25h[3d[4d[5d[6d[7d[8d[9d[10d[11d[12d[13d[14d[15d[16d[17d[18d[19d[20d[21d[22d[23d[24d[25d[26d[27d[28d[34d[K[29d[30d[31d[32d[33d7[3;34r8[34d[16S[1;36r[18;9H"/mnt/fass1/kirsten/databases/diamond_all/nr.dmnd"[19;5Hbenchmark:[20;9H"/mnt/fass1/kirsten/result/classification/benchmarks/diamond.db.benchmark.txt"[21;5Hconda:[22d"envs/diamond.yaml"[23;5Hparams:[24;9H"/mnt/fass1/kirsten/databases/diamond_all/nr"[25;5Hthreads: 8[26;5Hshell:[27d"diamond makedb --in {input.faa} -d {params} --taxonmap {input.map} --taxonnodes {input.nodes} --taxonnames {input.names}"[29drule centrifuge_db:[30;5Hinput:[31dmap = "/mnt/fass1/kirsten/databases/centrifuge_all/seqid2taxid.map",[32;9Hnodes = "/mnt/fass1/kirsten/databases/centrifuge_all/taxonomy/nodes.dmp",[33;9Hnames ="/mnt/fass1/kirsten/databases/centrifuge_all/taxonomy/names.dmp",[18d[19d[20d[21d[22d[23d[24d[25d[26d[27d[28d[29d[30d[31d[32d[33d7[3;34r8[34d[16S[1;36r[18;9Hfaa="/mnt/fass1/kirsten/databases/centrifuge_all/fna/full_bacteria_scerevisiae_cneoformans.fna"[19;5Houtput:[20;8Hfile1= "/mnt/fass1/kirsten/databases/centrifuge_all/bac_cer_neo.1.cf",[21;8Hfile2= "/mnt/fass1/kirsten/databases/centrifuge_all/bac_cer_neo.2.cf",[22;8Hfile3= "/mnt/fass1/kirsten/databases/centrifuge_all/bac_cer_neo.3.cf"[23;5Hthreads: 8[24;5Hbenchmark:[25;9H"/mnt/fass1/kirsten/result/classification/benchmarks/centrifuge.db.benchmark.txt"[26;5Hparams:[27;9H"/mnt/fass1/kirsten/databases/centrifuge_all/bar_cer_neo"[28;5Hconda:[29d"envs/centrifuge.yaml"[30;5Hshell:[31d"centrifuge-build -p {threads} --conversion-table {input.map} --taxonomy-tree {input.nodes} --name-table {input.names} {in$[33;1H# creating the project structure[18d[19d[3d[K[4d# creating the project structure[K[5drule create:[6;5Hshell:[K[7;5H    'python structure.py'[8d[K[9ddef get_run(wildcards): #returns the current value of variable/wildcard run[10;5Hreturn wildcards.run[11d[K[12ddef get_tool(wildcards): #returns the current value of variable/wildcard run[13d    return wildcards.tool[14d[K[15ddef get_medianHitLength(wildcards):[K[16;5Hreturn scripts.getting.get_seqLength(str(PATH[0])+"/data/"+str(wildcards.sample)+".fastq")#/2[17d #   elif str(wildcards.tool) == "kaiju":[K[18;3H#      lengths_default = {'gridion364':201, 'gridion366':194}[K[19;4H#     return lengths_default[str(wildcards.sample)]/2[20;5H# return wildcards.sample # scripts.getting.get_seqLength({PATH}+str(wildcards.sample)+".fastq")/2[21d[K[22drule centrifuge:[K[23;5Hinput:[K[24;5H    fastq = "{PATH}/data/{sample}.fastq"[25;5Houtput:[K[26;5H    files = "{PATH}/result/classification/centrifuge/{run}/{sample}_{run}.centrifuge.classification",[27;9Hreport= "{PATH}/result/classification/centrifuge/{run}/{sample}_{run}.centrifuge.report"[28;5Hbenchmark:[29;10H{PATH}/result/classification/benchmarks/{run}/{sample}_{run}.centrifuge.benchmark.txt"[30;5Hthreads: 8[31;5Hparams:[K[32;9Hrunid=get_run,[33;8H[1K db = DI["centrifuge"],  [3d[20;28r[20;1H[3T[1;36r[3;9Hrunid=get_run,[4;8H[1K db = DI["centrifuge"],  [5d#        medianlength=get_medianHitLength[6;5Hconda[7d"envs/centrifuge.yaml"[8;5Hrun:[9d[1K # -q[41m                            [49m(B[mfiles are fastq[K[10d[41m        [49m(B[m# -x[41m                            [49m(B[mindex files[11d[41m        [49m(B[m# --report-file[41m         [49m(B[mgenerated report file[12d[41m        [49m(B[m# -S[41m                            [49m(B[moutput file[K[13;5H    # -f[16X[13;29Hquery input files are (multi)fasta[14;9H# --ignore-quals[15d[K[16;5H    if 'default' in {params.runid}:[K[17;12H[1K shell('centrifuge -q -x {params.db} {input.fastq} --report-file {output.report} -S {output.files} --ignore-quals')[18;8H[1K elif 'quals' in {params.runid}:[K[19;12H[1K shell('centrifuge -q -x {params.db} {input.fastq} --report-file {output.report} -S {output.files}')[20;9Helif 'medianHitLength' in {params.runid}:[21;14Hshell('centrifuge -q -x {params.db} {input.fastq} --report-file {output.report} -S {output.files} --ignore-quals --mi$[22;9Helse:[23d[1K print("Centrifuge -- Nothing to be done here:", {params.runid})[K[25;6Hkraken2:[K[27;9Hdb = DI['kraken2'],[K[28;5H    files = "{PATH}/data/{sample}.fastq"[29;5Houtput:[K[30;5H    files = "{PATH}/result/classification/kraken2/{run}/{sample}_{run}.kraken2.classification",[31;5H    report= "{PATH}/result/classification/kraken2/{run}/{sample}_{run}.kraken2.report",[32;9Hunclassified="{PATH}/result/classification/kraken2/{run}/{sample}_{run}.kraken2.unclassified"[33;5Hbenchmark:[K[3d[22;33r[22;1H[4T[1;36r[15;24r[15;1H[4T[1;36r[3;9Hunclassified="{PATH}/result/classification/kraken2/{run}/{sample}_{run}.kraken2.unclassified"[4;5Hbenchmark:[K[5;8H[1K "{PATH}/result/classification/benchmarks/{run}/{sample}_{run}.kraken2.benchmark.txt"[6;5Hthreads: 8[7;5Hparams:[K[8;5H    runid=get_run[9;5Hlog:[K[10d[1K '{PATH}/result/classification/kraken2/{run}/{sample}_{run}.kraken2.log'[11d    conda:[K[12;8H[1K 'envs/main.yaml'[K[13;5Hrun:[K[14;13Hconfidence  [14;33Hthreshold that must be in [0,1][15;9H# --unclassified-out    prints unclassified sequences to filename[16;9H# --classified-out[33Gprints classified sequences to filename[17;9H# --output[17;33Hprints output to filename[18;9H# --report[18;33Hprints report with aggregate counts/clade to file[21;20Hkraken2 --db {input.db} --unclassified-out {output.unclassified} --report {output.report} --threads {threads} -$[22;15Hmedium' in {params.runid}:#confidence set[23;20Hkraken2 --db {input.db} --report {output.report} --confidence 0.05 --threads {threads} --output {output.files} $[24;15Hrestrictive[4P[25;13Hprint("Sure")[27;20HKraken2 -- Nothing to do[8P[29;7Haiju:[K[31;19Haiju']+"/kaiju_db_refseq.fmi",[32;9Hnodes = DI['kaiju']+"/nodes.dmp",   [33;5H    files = "{PATH}/data/{sample}.fastq"[3d[19;33r[19;1H[4T[1;36r[9;18r[9;1H[4T[1;36r[2;11r[2;1H[3T[1;36r[3;9Hnodes = DI['kaiju']+"/nodes.dmp",[4;9Hfiles = "{PATH}/data/{sample}.fastq"[5;5Houtput:[6;9Hfiles = '{PATH}/result/classification/kaiju/{run}/{sample}_{run}.kaiju.classification'[K[8;9H'[8;72Haiju.benchmark.txt'  [9;14H4[11;22H,[12;9HmedianHitLength=get_medianHitLength[14;40Haiju/{run}/{sample}_{run}.kaiju[4P[18;12Ht    name of nodes.dmp file[K[19;9H# -f    name of database (.fmi) file[20;9H# -i    input file containing fasta/fastq[21;9H# -o    name of output file[22;9H# -m    minimum match length (default: 11)[23;9H# -E    minimum e-value in Greedy mode (which is default)[25;21Haiju -t {input.nodes} -f {input.db} -i {input.files} -o {output.files} -z {threads} -v')[K[26;19HanHitLength' in {params.runid}:[K[27;21Haiju -t {input.nodes} -f {input.db} -i {input.files} -o {output.files} -z {threads} -v -m {params.medianHitLen[29;14Hass[K[31;21Haiju[2P[33;11H_summary:[3d[9;18r[18;1H[4S[1;36r[3;1H[K[4drule kaiju_summary:[K[5;5Hin[1P[6d  nodes = DI['kaiju']+"/nodes.dmp",[K[7;5H    names= DI['kaiju']+"/names.dmp",[8;9Hfiles = "{PATH}/result/classification/kaiju/{run}/{sample}_{run}.kaiju.classification"[9;5Houtput:[10;9H"[10;72Hreport"[13;5Hshell:[14d"kaiju2table -t {input.nodes} -n {input.names} -r species -o {output} {input.files}"[16drule taxmaps: # many folders, fix output[17;5Hinput:[18ddb = DI['taxmaps']+"/refseq_complete_bacarchvir.lcak300.gem",[19;9Htaxonomy = DI['taxmaps']+"/taxonomy.tbl.gz",[20;9Hnodes = DI['kaiju']+"/nodes.dmp",[K[21;9Hfiles = "{PATH}/data/{sample}.fastq"[22;5Hbenchmark:[K[23;9H'{PATH}/result/classification/benchmarks/{run}/{sample}_{run}.taxmaps.benchmark.txt'[24;5Houtput:[K[25;9Ho = '{PATH}/result/classification/taxmaps/{run}/{sample}_{run}.taxmaps.classification',[K[26;9Hfolder = '{PATH}/result/classification/taxmaps/{run}/{sample}'[27;5Hthreads: 8[K[28;5Hparams:[K[29;9Hrunid=get_run,[30;9Hprefix = "{sample}_{run}.taxmaps.classification"[31;5Hlog:[K[32d"{PATH}/result/classification/taxmaps/{run}/{sample}_{run}.taxmaps.log"[33d    conda:[K[3d[3;9H"{PATH}/result/classification/taxmaps/{run}/{sample}_{run}.taxmaps.log"[4d    conda:[K[5;5H    'envs/taxmaps.yaml'[6;5Hrun:[K[7d# -f        input fastq[K[8;9H# -l        in preprocessing: minimum read length for mapping[K[9;5H    # -C[9;21Hin preprocessing: entropy cutoff for low complexity filtering[10;9H# -d        index files[K[11;5H    # -t[11;21Htaxonomic rable[12;9H# --cov     coverage histogram[13;5H    # -o[13;21Houtput directory[14;9Hif 'default' in {params.runid}:[K[15;13Hshell('export PERL5LIB=/home/re85gih/miniconda3/envs/taxmaps/opt/krona/lib/'),[16;12H[1K shell('export PATH=$PATH:/home/re85gih/projectClassification/taxmaps/'),[17;12H[1K shell('taxMaps -f {input.files} -c {threads} -t {input.taxonomy} -d {input.db} -o {output.folder} -p {params.prefix}')[18;9Helif 'medium' in {params.runid}:[K[19;9H    pass[K[20;9Helif 'restrictive' in {params.runid}:[21;9H    pass[K[22;5H    else:[K[23;9H    print("TaxMaps -- Nothing to do here:", {params.runid})[K[24d[K[25drule deepmicrobes:[K[26;5Hinput:[K[27;5H    files = "{PATH}/data/{sample}.fastq",[28;5H    kmers =DI["deepmicrobes"]+"/tokens_merged_12mers.txt",[29;9Hweights=DI["deepmicrobes"]+"/weights_species",[30;9Hname2label=DI["deepmicrobes"]+"/name2label_species.txt"[31;5Houtput:[32;9Hprediction="{PATH}/result/classification/deepmicrobes/{run}/{sample}_{run}.deepmicrobes.prediction.tfrec",[33;5H    tfrec="{PATH}/result/classification/deepmicrobes/{run}/{sample}_{run}.deepmicrobes.training.tfrec",[3d[2;10r[2;1H[3T[1;36r[3;9Hprediction="{PATH}/result/classification/deepmicrobes/{run}/{sample}_{run}.deepmicrobes.prediction.tfrec",[4;9Htfrec="{PATH}/result/classification/deepmicrobes/{run}/{sample}_{run}.deepmicrobes.training.tfrec",[5;9Hclassification="{PATH}/result/classification/deepmicrobes/{run}/{sample}_{run}.deepmicrobes.classification",[6;9Hreport="{PATH}/result/classification/deepmicrobes/{run}/{sample}_{run}.deepmicrobes.report"[8;15Hdeepmicrobes.yaml'[9;5Hbenchmark:[10;9H"{PATH}/result/classification/benchmarks/{run}/{sample}_{run}.deepmicrobes.benchmark.txt"[11;5Hthreads: 8[K[12;5Hrun:[K[13;12H-kmer        length of k-mers (default: 12) --> if i want to change that i might need to build my own index[14;9H# --max_len     max length of sequences (default: 150)[15;9H# --pred_out    path to prediction output[K[16;9H# -dd           location of input data[K[17;9H# -ebe          number of training epochs to run between evaluations[K[18;9H# just for me now[K[19d[K[20;9Hshell('export PATH=/home/re85gih/projectClassification/DeepMicrobes/pipelines:$PATH'),[21;9Hshell('export PATH=/home/re85gih/projectClassification/DeepMicrobes/scripts:$PATH'),[22;9Hshell('export PATH=/home/re85gih/projectClassification/DeepMicrobes:$PATH'),[23d[K[24;9H# transform training fastq to tfrec[25;8H[1K #shell('tfrec_train_kmer.sh -i {input.files} -v {input.kmers} -o {output.tfrec}'),[26d[K[27;9H# transform prediction fastq to tfrec[28;9Hshell('tfrec_predict_kmer.sh -f {input.files} -t fastq -v {input.kmers} -o {output.prediction}'),[29d[K[30;9H# make prediction on metagenome datasaet[K[31;5H    shell('predict_DeepMicrobes.sh -i {output.prediction} -l species -p 8 -m {input.weights} -o {output.classification}'),[32d[K[33;9H# generate taxonomic profiles[K[3d[6;16r[6;1H[4T[1;36r[3;1H[K[4;9H# generate taxonomic profiles[K[5;9Hshell('report_profile.sh -i {output.classification} -o {output.report} -t 50 -l {input.name2label}')[K[7drule kslam:[8;5Hinput:[9ddb = DI['kslam'],[10;9Hfiles = "{PATH}/data/{sample}.fastq"[K[11;5Houtput:[12;9H"{PATH}/result/classification/kslam/{run}/{sample}_{run}.kslam.classification"[14;71Hkslam[7P[15;14H16[16;5Hparams:[17;9Hrunid=get_run[K[18;5Hlog:[K[19d"{PATH}/result/classification/kslam/{run}/{sample}_{run}.kslam.log"[20;5Hconda:[K[21d'envs/kslam.yaml'[K[22;5Hrun:[K[23d# --db[23;37Hdatabase file[24;11H--min-alignment-score     alignment score cutoff[25;9Hif 'default' in {params.runid}:[K[26;13Hshell('SLAM --db={input.db} --output-file={output} --num-reads-at-once 1000000 {input.files}')[27;9Helif 'medium' in {params.runid}:[K[28;9H    pass[K[29;9Helif 'restrictive' in {params.runid}:[30;9H    pass[K[31;9Helse:[K[32dprint("KSLAM -- Nothing to do here:", {params.runid})[33d[K[3d[4;9r[9;1H[2S[1;36r[8;12r[12;1H
[1;36r[25;33r[25;1H[3T[1;36r[17;26r[17;1HM[1;36r[3;13Hprint("KSLAM -- Nothing to do here:", {params.runid})[5;6Hclark: #output is csv, watch out[7;18Hclark']+"/",[8;9Hfiles = "{PATH}/data/{sample}.fastq",[9;9Htargets = "/mnt/fass1/database/clark_database/targets.txt"[11;9Hhelper = "{PATH}/result/classification/clark/{run}/{sample}_{run}.clark.classification.csv",[12;9Hfiles= "{PATH}/result/classification/clark/{run}/{sample}_{run}.clark.classification"[14;71Hclark[15;14H8 [17;13Hdb = "/mnt/fass1/kirsten/database/clark/",[18d[18;9H    runid=get_run[20;39Hclark[20;66Hclark[22;15Hmain[1P[24;12Hk  [21Gk-mer size, has to be between 2 and 32, default:31[25;13Hlong    for long reads (only for full mode)   [26;9H# -m[26;21Hmode of execution[29;20HCLARK --long -O {input.files} -R {output.files} -D {input.db} -n {threads} -T {input.targets}')[3d[4d[5d[6d[7d[8d[9d[10d[11d[12d[13d[14d[15d[16d[17d[18d[19d[20d[21d[22d[23d[24d[25d[26d[27d[28d[29d            shell('CLARK --long -O {input.files} -R {output.files} -D {input[1;122H(B[0;7mModified[29;76H(B[m[1P[1P[1P[1P[1Pp.db} -n {threads} -T {input.targets}')[29;73Ha.db} -n {threads} -T {input.targets}')[29;74Hr.db} -n {threads} -T {input.targets}')[29;75Ha.db} -n {threads} -T {input.targets}')[29;76Hm.db} -n {threads} -T {input.targets}')[29;77Hs.db} -n {threads} -T {input.targets}')[29;78H[34d(B[0;7mSave modified buffer?  (Answering "No" will DISCARD changes.)                                                                      [35;1H Y(B[m Yes[K[36d(B[0;7m N(B[m No  [36;18H(B[0;7mC(B[m Cancel[K[34;63H[35d(B[0;7m^G(B[m Get Help[35;33H(B[0;7mM-D(B[m DOS Format[35;65H(B[0;7mM-A(B[m Append[35;97H(B[0;7mM-B(B[m Backup File[36d(B[0;7m^C(B[m Cancel[17G         [33G(B[0;7mM-M(B[m Mac Format[36;65H(B[0;7mM-P(B[m Prepend[36;97H(B[0;7m^T(B[m To Files[34d(B[0;7mFile Name to Write: Snakefile                                (B[m[34;30H[?25l[34;56H[1K (B[0;7m[ Wrote 527 lines ](B[m[K[J[1;122H(B[0;7m        [36;131H(B[m[36;1H[?12l[?25h[?1049l[?1l>(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:12:33 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0mnano Snakefile [2@config.yaml[C[2PSnakefile[Csnakemake --use-conda -j 8
[31mMissingInputException in line 296 of /data/fass1/kirsten/result/classificationBenchmark/Snakefile:
Missing input files for rule clark:
/mnt/fass1/kirsten/database/clark/[0m
(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:12:37 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0msnakemake --use-conda -j 8[11Pnano Snakefile 
[?1049h[1;36r(B[m[4l[?7h[?1h=[?1h=[?1h=[39;49m[?25l[39;49m(B[m[H[2J[34;58H(B[0;7m[ Reading File ](B[m[34;57H(B[0;7m[ Read 527 lines ](B[m[H[39;49m(B[0;7m  GNU nano 2.7.4                                             File: Snakefile                                                       [3;1H(B[m# Snakefile for project work of benchmarking different classification tools regarding their usability for long reads[5dconfigfile: "config.yaml"[7d########## VARIABLE DEFINITION[8dDI= dict(config["dataIndex"])[9dPATH = config["path"][10dSAMPLES = "gridion366" #list(config["samples"])[11dTOOLS= 'clark'#ccmetagen centrifuge kraken2 clark kaiju'.split(" ") #list(config["classification"])[12dRUNS='default'#'medianHitLength'#'quals'# 'default medium restrictive'.split()[14dimport scripts.getting[17drule all:[18;5Hinput:[19d# CLASSIFICATION[20;9Hexpand("{path}/result/classification/{tool}/{run}/{sample}_{run}.{tool}.classification", run=RUNS, sample=SAMPLES, tool=TO$[21;1H# GENERATING (comparable) REPORTS[22d#[9Gexpand("{path}/result/classification/{tool}/{run}/{sample}_{run}.{tool}.areport", run=RUNS, sample=SAMPLES, tool=TOOLS, pa$[23;1H#[23;10Hexpand("{path}/result/classification/{tool}/{run}/{sample}_{run}.{tool}.report", run=RUNS, sample=SAMPLES, tool=TOOLS, pa$[24;1H#[9Gexpand("{path}/result/classification/ccmetagen/{sample}.{tool}.intermediate.res", run=RUNS, sample=SAMPLES, tool=TOOLS, pa$[25;1H#[9Gexpand("{path}/result/classification/{tool}/default/{sample}_{run}.catbat.bins",tool=TOOLS, run=RUNS, sample=SAMPLES, path$[27;1Hrule diamond_db:[28;5Hinput:[29dfaa="/mnt/fass1/kirsten/databases/diamond_all/faa/full_proteome_bacteria_scerevisiae_cneoformans.faa",[30;9Hmap="/mnt/fass1/genomes/new_bacteria/bacteria_blast_db/prot_accession2taxid.txt",[31;9Hnodes="/mnt/fass1/kirsten/databases/diamond/nodes.dmp",[32;9Hnames="/mnt/fass1/kirsten/databases/diamond/names.dmp"[33;5Houtput:[35d(B[0;7m^G(B[m Get Help     (B[0;7m^O(B[m Write Out    (B[0;7m^W(B[m Where Is     (B[0;7m^K(B[m Cut Text     (B[0;7m^J(B[m Justify[81G(B[0;7m^C(B[m Cur Pos[97G(B[0;7m^Y(B[m Prev Page    (B[0;7mM-\(B[m First Line[36d(B[0;7m^X(B[m Exit[36;17H(B[0;7m^R(B[m Read File    (B[0;7m^\(B[m Replace[49G(B[0;7m^U(B[m Uncut Text   (B[0;7m^T(B[m To Spell     (B[0;7m^_(B[m Go To Line   (B[0;7m^V(B[m Next Page    (B[0;7mM-/(B[m Last Line[34d[?12l[?25h[3d[8G[1K names="/mnt/fass1/kirsten/databases/diamond/names.dmp"[K[4;5Houtput:[5;8H[1K "/mnt/fass1/kirsten/databases/diamond_all/nr.dmnd"[6;5Hbenchmark:[7;8H[1K "/mnt/fass1/kirsten/result/classification/benchmarks/diamond.db.benchmark.txt"[8d    conda:[K[9;8H[1K "envs/diamond.yaml"[10d    params:[K[11;8H[1K "/mnt/fass1/kirsten/databases/diamond_all/nr"[K[12d    threads: 8[K[13;5Hshell:[14;8H[1K "diamond makedb --in {input.faa} -d {params} --taxonmap {input.map} --taxonnodes {input.nodes} --taxonnames {input.names}"[16drule centrifuge_db:[17d    input:[18;5H    map = "/mnt/fass1/kirsten/databases/centrifuge_all/seqid2taxid.map",[19;8H[1K nodes = "/mnt/fass1/kirsten/databases/centrifuge_all/taxonomy/nodes.dmp",[20;9Hnames ="/mnt/fass1/kirsten/databases/centrifuge_all/taxonomy/names.dmp",[K[21;8H[1K faa="/mnt/fass1/kirsten/databases/centrifuge_all/fna/full_bacteria_scerevisiae_cneoformans.fna"[22d    output:[K[23;7H[1K file1= "/mnt/fass1/kirsten/databases/centrifuge_all/bac_cer_neo.1.cf",[K[24;7H[1K file2= "/mnt/fass1/kirsten/databases/centrifuge_all/bac_cer_neo.2.cf",[K[25;7H[1K file3= "/mnt/fass1/kirsten/databases/centrifuge_all/bac_cer_neo.3.cf"[K[26;5Hthreads: 8[27d    benchmark:[K[28;5H    "/mnt/fass1/kirsten/result/classification/benchmarks/centrifuge.db.benchmark.txt"[29;5Hparams:[K[30;9H"/mnt/fass1/kirsten/databases/centrifuge_all/bar_cer_neo"[K[31;5Hconda:[K[32d"envs/centrifuge.yaml"[K[33;5Hshell:[K[3d[14;33r[14;1H[9T[1;36r[3;9H"envs/centrifuge.yaml"[K[4;5Hshell:[K[5dcentrifuge-build -p {threads} --conversion-table {input.map} --taxonomy-tree {input.nodes} --name-table {input.names} {in$[6;1H[K[7d# creating the project structure[K[8drule create:[9;5Hshell:[K[10;5H    'python structure.py'[11d[K[12ddef get_run(wildcards): #returns the current value of variable/wildcard run[13;5Hreturn wildcards.run[15ddef get_tool(wildcards): #returns the current value of variable/wildcard run[16;5Hreturn wildcards.tool[18ddef get_medianHitLength(wildcards):[19;5Hreturn scripts.getting.get_seqLength(str(PATH[0])+"/data/"+str(wildcards.sample)+".fastq")#/2[20d #   elif str(wildcards.tool) == "kaiju":[21;3H#[10Glengths_default = {'gridion364':201, 'gridion366':194}[22;4H#     return lengths_default[str(wildcards.sample)]/2[23;5H# return wildcards.sample # scripts.getting.get_seqLength({PATH}+str(wildcards.sample)+".fastq")/2[K[25;16H:[K[27;9Hfastq = "{PATH}/data/{sample}.fastq"[K[28;5Houtput:[K[29;9Hfiles = "{PATH}/result/classification/centrifuge/{run}/{sample}_{run}.centrifuge.classification",[30;9Hreport= "{PATH}/result/classification/centrifuge/{run}/{sample}_{run}.centrifuge.report"[K[31;5Hbenchmark:[32;8H "{PATH}/result/classification/benchmarks/{run}/{sample}_{run}.centrifuge.benchmark.txt"[33;5Hthreads: 8[K[3d[23;31r[23;1H[3T[1;36r[3;10H{PATH}/result/classification/benchmarks/{run}/{sample}_{run}.centrifuge.benchmark.txt"[4;5Hthreads: 8[5;5Hparams:[K[6;9Hrunid=get_run,[7;8H[1K db = DI["centrifuge"],  [8d#        medianlength=get_medianHitLength[9;5Hconda[10d"envs/centrifuge.yaml"[11;5Hrun:[12d[1K # -q[41m                            [49m(B[mfiles are fastq[K[13d[41m        [49m(B[m# -x[41m                            [49m(B[mindex files[14d[41m        [49m(B[m# --report-file[41m         [49m(B[mgenerated report file[15d[41m        [49m(B[m# -S[41m                            [49m(B[moutput file[K[16;5H    # -f[16X[16;29Hquery input files are (multi)fasta[17;9H# --ignore-quals[18d[K[19;5H    if 'default' in {params.runid}:[K[20;12H[1K shell('centrifuge -q -x {params.db} {input.fastq} --report-file {output.report} -S {output.files} --ignore-quals')[21;8H[1K elif 'quals' in {params.runid}:[K[22;12H[1K shell('centrifuge -q -x {params.db} {input.fastq} --report-file {output.report} -S {output.files}')[23;9Helif 'medianHitLength' in {params.runid}:[24;14Hshell('centrifuge -q -x {params.db} {input.fastq} --report-file {output.report} -S {output.files} --ignore-quals --mi$[25;9Helse:[26d[1K print("Centrifuge -- Nothing to be done here:", {params.runid})[K[28;6Hkraken2:[K[30;9Hdb = DI['kraken2'],[K[31;5H    files = "{PATH}/data/{sample}.fastq"[32;5Houtput:[K[33;5H    files = "{PATH}/result/classification/kraken2/{run}/{sample}_{run}.kraken2.classification",[3d[25;33r[25;1H[4T[1;36r[18;27r[18;1H[4T[1;36r[2;16r[2;1H[5T[1;36r[3;5Houtput:[4;9Hfiles = "{PATH}/result/classification/kraken2/{run}/{sample}_{run}.kraken2.classification",[5;9Hreport= "{PATH}/result/classification/kraken2/{run}/{sample}_{run}.kraken2.report",[6;9Hunclassified="{PATH}/result/classification/kraken2/{run}/{sample}_{run}.kraken2.unclassified"[7;5Hbenchmark:[8;71Hkraken2[3P[11;22H [12;5Hlog:[K[13d[1K '{PATH}/result/classification/kraken2/{run}/{sample}_{run}.kraken2.log'[15;8H 'envs/main.yaml'[K[17;13Hconfidence  [17;33Hthreshold that must be in [0,1][18;9H# --unclassified-out    prints unclassified sequences to filename[19;9H# --classified-out[33Gprints classified sequences to filename[20;9H# --output[20;33Hprints output to filename[21;9H# --report[21;33Hprints report with aggregate counts/clade to file[24;20Hkraken2 --db {input.db} --unclassified-out {output.unclassified} --report {output.report} --threads {threads} -$[25;15Hmedium' in {params.runid}:#confidence set[26;20Hkraken2 --db {input.db} --report {output.report} --confidence 0.05 --threads {threads} --output {output.files} $[27;15Hrestrictive[4P[28;13Hprint("Sure")[30;20HKraken2 -- Nothing to do[8P[32;7Haiju:[K[3d[22;33r[22;1H[4T[1;36r[12;21r[12;1H[4T[1;36r[6;14r[6;1H[3T[1;36r[3;1Hrule kaiju[4;5Hinput:[K[5ddb = DI['kaiju']+"/kaiju_db_refseq.fmi",[K[6;9Hnodes = DI['kaiju']+"/nodes.dmp",[7;9Hfiles = "{PATH}/data/{sample}.fastq"[8;5Houtput:[9;9Hfiles = '{PATH}/result/classification/kaiju/{run}/{sample}_{run}.kaiju.classification'[K[11;9H'[11;72Haiju.benchmark.txt'  [12;14H4[14;22H,[15;9HmedianHitLength=get_medianHitLength[17;40Haiju/{run}/{sample}_{run}.kaiju[4P[21;12Ht    name of nodes.dmp file[K[22;9H# -f    name of database (.fmi) file[23;9H# -i    input file containing fasta/fastq[24;9H# -o    name of output file[25;9H# -m    minimum match length (default: 11)[26;9H# -E    minimum e-value in Greedy mode (which is default)[28;21Haiju -t {input.nodes} -f {input.db} -i {input.files} -o {output.files} -z {threads} -v')[K[29;19HanHitLength' in {params.runid}:[K[30;21Haiju -t {input.nodes} -f {input.db} -i {input.files} -o {output.files} -z {threads} -v -m {params.medianHitLen[32;14Hass[K[3d[12;21r[21;1H[4S[1;36r[3;12H[1K pass[4;5H    else:[5;9H    print("Kaiju -- Nothing to do here:", {params.runid})[6d[K[7drule kaiju_summary:[K[8;5Hin[1P[9d  nodes = DI['kaiju']+"/nodes.dmp",[K[10;5H    names= DI['kaiju']+"/names.dmp",[11;9Hfiles = "{PATH}/result/classification/kaiju/{run}/{sample}_{run}.kaiju.classification"[12;5Houtput:[13;9H"[13;72Hreport"[16;5Hshell:[17d"kaiju2table -t {input.nodes} -n {input.names} -r species -o {output} {input.files}"[19drule taxmaps: # many folders, fix output[20;5Hinput:[21ddb = DI['taxmaps']+"/refseq_complete_bacarchvir.lcak300.gem",[22;9Htaxonomy = DI['taxmaps']+"/taxonomy.tbl.gz",[23;9Hnodes = DI['kaiju']+"/nodes.dmp",[K[24;9Hfiles = "{PATH}/data/{sample}.fastq"[25;5Hbenchmark:[K[26;9H'{PATH}/result/classification/benchmarks/{run}/{sample}_{run}.taxmaps.benchmark.txt'[27;5Houtput:[K[28;9Ho = '{PATH}/result/classification/taxmaps/{run}/{sample}_{run}.taxmaps.classification',[K[29;9Hfolder = '{PATH}/result/classification/taxmaps/{run}/{sample}'[30;5Hthreads: 8[K[31;5Hparams:[K[32;9Hrunid=get_run,[33;9Hprefix = "{sample}_{run}.taxmaps.classification"[3d[3;9Hrunid=get_run,[4;9Hprefix = "{sample}_{run}.taxmaps.classification"[5;5Hlog:[K[6d"{PATH}/result/classification/taxmaps/{run}/{sample}_{run}.taxmaps.log"[7d    conda:[K[8;5H    'envs/taxmaps.yaml'[9;5Hrun:[K[10d# -f        input fastq[K[11;9H# -l        in preprocessing: minimum read length for mapping[K[12;5H    # -C[12;21Hin preprocessing: entropy cutoff for low complexity filtering[13;9H# -d        index files[K[14;5H    # -t[14;21Htaxonomic rable[15;9H# --cov     coverage histogram[16;5H    # -o[16;21Houtput directory[17;9Hif 'default' in {params.runid}:[K[18;13Hshell('export PERL5LIB=/home/re85gih/miniconda3/envs/taxmaps/opt/krona/lib/'),[19;12H[1K shell('export PATH=$PATH:/home/re85gih/projectClassification/taxmaps/'),[20;12H[1K shell('taxMaps -f {input.files} -c {threads} -t {input.taxonomy} -d {input.db} -o {output.folder} -p {params.prefix}')[21;9Helif 'medium' in {params.runid}:[K[22;9H    pass[K[23;9Helif 'restrictive' in {params.runid}:[24;9H    pass[K[25;5H    else:[K[26;9H    print("TaxMaps -- Nothing to do here:", {params.runid})[K[27d[K[28drule deepmicrobes:[K[29;5Hinput:[K[30;5H    files = "{PATH}/data/{sample}.fastq",[31;5H    kmers =DI["deepmicrobes"]+"/tokens_merged_12mers.txt",[32;9Hweights=DI["deepmicrobes"]+"/weights_species",[33;9Hname2label=DI["deepmicrobes"]+"/name2label_species.txt"[3d[6;13r[6;1H[3T[1;36r[3;9Hweights=DI["deepmicrobes"]+"/weights_species",[4;9Hname2label=DI["deepmicrobes"]+"/name2label_species.txt"[5;5Houtput:[6;9Hprediction="{PATH}/result/classification/deepmicrobes/{run}/{sample}_{run}.deepmicrobes.prediction.tfrec",[7;9Htfrec="{PATH}/result/classification/deepmicrobes/{run}/{sample}_{run}.deepmicrobes.training.tfrec",[8;9Hclassification="{PATH}/result/classification/deepmicrobes/{run}/{sample}_{run}.deepmicrobes.classification",[9;9Hreport="{PATH}/result/classification/deepmicrobes/{run}/{sample}_{run}.deepmicrobes.report"[11;15Hdeepmicrobes.yaml'[12;5Hbenchmark:[13;9H"{PATH}/result/classification/benchmarks/{run}/{sample}_{run}.deepmicrobes.benchmark.txt"[14;5Hthreads: 8[K[15;5Hrun:[K[16;12H-kmer        length of k-mers (default: 12) --> if i want to change that i might need to build my own index[17;9H# --max_len     max length of sequences (default: 150)[18;9H# --pred_out    path to prediction output[K[19;9H# -dd           location of input data[K[20;9H# -ebe          number of training epochs to run between evaluations[K[21;9H# just for me now[K[22d[K[23;9Hshell('export PATH=/home/re85gih/projectClassification/DeepMicrobes/pipelines:$PATH'),[24;9Hshell('export PATH=/home/re85gih/projectClassification/DeepMicrobes/scripts:$PATH'),[25;9Hshell('export PATH=/home/re85gih/projectClassification/DeepMicrobes:$PATH'),[26d[K[27;9H# transform training fastq to tfrec[28;8H[1K #shell('tfrec_train_kmer.sh -i {input.files} -v {input.kmers} -o {output.tfrec}'),[29d[K[30;9H# transform prediction fastq to tfrec[31;9Hshell('tfrec_predict_kmer.sh -f {input.files} -t fastq -v {input.kmers} -o {output.prediction}'),[32d[K[33;9H# make prediction on metagenome datasaet[K[3d[9;19r[9;1H[4T[1;36r[3;1H[K[4;9H# make prediction on metagenome datasaet[K[5;5H    shell('predict_DeepMicrobes.sh -i {output.prediction} -l species -p 8 -m {input.weights} -o {output.classification}'),[6d[K[7;9H# generate taxonomic profiles[K[8;9Hshell('report_profile.sh -i {output.classification} -o {output.report} -t 50 -l {input.name2label}')[K[10drule kslam:[11;5Hinput:[12ddb = DI['kslam'],[13;9Hfiles = "{PATH}/data/{sample}.fastq"[K[14;5Houtput:[15;9H"{PATH}/result/classification/kslam/{run}/{sample}_{run}.kslam.classification"[17;71Hkslam[7P[18;14H16[19;5Hparams:[20;9Hrunid=get_run[K[21;5Hlog:[K[22d"{PATH}/result/classification/kslam/{run}/{sample}_{run}.kslam.log"[23;5Hconda:[K[24d'envs/kslam.yaml'[K[25;5Hrun:[K[26d# --db[26;37Hdatabase file[27;11H--min-alignment-score     alignment score cutoff[28;9Hif 'default' in {params.runid}:[K[29;13Hshell('SLAM --db={input.db} --output-file={output} --num-reads-at-once 1000000 {input.files}')[30;9Helif 'medium' in {params.runid}:[K[31;9H    pass[K[32;9Helif 'restrictive' in {params.runid}:[33;9H    pass[K[3d[7;12r[12;1H[2S[1;36r[11;15r[15;1H
[1;36r[28;33r[28;1H[3T[1;36r[20;29r[20;1HM[1;36r[3;9Helif 'restrictive' in {params.runid}:[4;9H    pass[K[5;9Helse:[K[6dprint("KSLAM -- Nothing to do here:", {params.runid})[8;6Hclark: #output is csv, watch out[10;18Hclark']+"/",[11;9Hfiles = "{PATH}/data/{sample}.fastq",[12;9Htargets = "/mnt/fass1/database/clark_database/targets.txt"[14;9Hhelper = "{PATH}/result/classification/clark/{run}/{sample}_{run}.clark.classification.csv",[15;9Hfiles= "{PATH}/result/classification/clark/{run}/{sample}_{run}.clark.classification"[17;71Hclark[18;14H8 [20;13Hdb = "/mnt/fass1/kirsten/database/clark/",[21d[21;9H    runid=get_run[23;39Hclark[23;66Hclark[25;15Hmain[1P[27;12Hk  [21Gk-mer size, has to be between 2 and 32, default:31[28;13Hlong    for long reads (only for full mode)   [29;9H# -m[29;21Hmode of execution[32;20HCLARK --long -O {input.files} -R {output.files} -D {params.db} -n {threads} -T {input.targets}')[3d[4d[5d[6d[7d[8d[9d[10d        [1;122H(B[0;7mModified[10;9H(B[m#db = DI['clark']+"/",[34d[K[10;10Hdb = DI['clark'[34d(B[0;7mSave modified buffer?  (Answering "No" will DISCARD changes.)                                                                      [35;1H Y(B[m Yes[K[36d(B[0;7m N(B[m No  [36;18H(B[0;7mC(B[m Cancel[K[34;63H[35d(B[0;7m^G(B[m Get Help[35;33H(B[0;7mM-D(B[m DOS Format[35;65H(B[0;7mM-A(B[m Append[35;97H(B[0;7mM-B(B[m Backup File[36d(B[0;7m^C(B[m Cancel[17G         [33G(B[0;7mM-M(B[m Mac Format[36;65H(B[0;7mM-P(B[m Prepend[36;97H(B[0;7m^T(B[m To Files[34d(B[0;7mFile Name to Write: Snakefile                                (B[m[34;30H[?25l[34;56H[1K (B[0;7m[ Wrote 527 lines ](B[m[K[J[1;122H(B[0;7m        [36;131H(B[m[36;1H[?12l[?25h[?1049l[?1l>(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:13:02 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0mnano Snakefile snakemake --use-conda -j 8
[33mProvided cores: 8[0m
[33mRules claiming more threads will be scaled down.[0m
[33mJob counts:
	count	jobs
	1	all
	1	clark
	2[0m
[32m[0m
[32mrule clark:
    input: /mnt/fass1/database/clark_database/targets.txt, /mnt/fass1/kirsten/data/gridion366.fastq
    output: /mnt/fass1/kirsten/result/classification/clark/default/gridion366_default.clark.classification.csv, /mnt/fass1/kirsten/result/classification/clark/default/gridion366_default.clark.classification
    log: /mnt/fass1/kirsten/result/classification/clark/default/gridion366_default.clark.log
    jobid: 1
    benchmark: /mnt/fass1/kirsten/result/classification/benchmarks/default/gridion366_default.clark.benchmark.txt
    wildcards: run=default, sample=gridion366, PATH=/mnt/fass1/kirsten
    threads: 8[0m
[32m[0m
Failed to find/read the directory:  /mnt/fass1/kirsten/database/clark/
[31mError in job clark while creating output files /mnt/fass1/kirsten/result/classification/clark/default/gridion366_default.clark.classification.csv, /mnt/fass1/kirsten/result/classification/clark/default/gridion366_default.clark.classification.[0m
[31mRuleException:
CalledProcessError in line 320 of /data/fass1/kirsten/result/classificationBenchmark/Snakefile:
Command 'CLARK --long -O /mnt/fass1/kirsten/data/gridion366.fastq -R /mnt/fass1/kirsten/result/classification/clark/default/gridion366_default.clark.classification -D /mnt/fass1/kirsten/database/clark/ -n 8 -T /mnt/fass1/database/clark_database/targets.txt' returned non-zero exit status 1
  File "/data/fass1/kirsten/result/classificationBenchmark/Snakefile", line 320, in __rule_clark
  File "/usr/lib/python3.5/concurrent/futures/thread.py", line 55, in run[0m
[31mExiting because a job execution failed. Look above for error message[0m
[33mWill exit after finishing currently running jobs.[0m
[31mExiting because a job execution failed. Look above for error message[0m
(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:13:06 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0mnano Snakefile 
[?1049h[1;36r(B[m[4l[?7h[?1h=[?1h=[?1h=[39;49m[?25l[39;49m(B[m[H[2J[34;58H(B[0;7m[ Reading File ](B[m[34;57H(B[0;7m[ Read 527 lines ](B[m[H[39;49m(B[0;7m  GNU nano 2.7.4                                             File: Snakefile                                                       [3;1H(B[m# Snakefile for project work of benchmarking different classification tools regarding their usability for long reads[5dconfigfile: "config.yaml"[7d########## VARIABLE DEFINITION[8dDI= dict(config["dataIndex"])[9dPATH = config["path"][10dSAMPLES = "gridion366" #list(config["samples"])[11dTOOLS= 'clark'#ccmetagen centrifuge kraken2 clark kaiju'.split(" ") #list(config["classification"])[12dRUNS='default'#'medianHitLength'#'quals'# 'default medium restrictive'.split()[14dimport scripts.getting[17drule all:[18;5Hinput:[19d# CLASSIFICATION[20;9Hexpand("{path}/result/classification/{tool}/{run}/{sample}_{run}.{tool}.classification", run=RUNS, sample=SAMPLES, tool=TO$[21;1H# GENERATING (comparable) REPORTS[22d#[9Gexpand("{path}/result/classification/{tool}/{run}/{sample}_{run}.{tool}.areport", run=RUNS, sample=SAMPLES, tool=TOOLS, pa$[23;1H#[23;10Hexpand("{path}/result/classification/{tool}/{run}/{sample}_{run}.{tool}.report", run=RUNS, sample=SAMPLES, tool=TOOLS, pa$[24;1H#[9Gexpand("{path}/result/classification/ccmetagen/{sample}.{tool}.intermediate.res", run=RUNS, sample=SAMPLES, tool=TOOLS, pa$[25;1H#[9Gexpand("{path}/result/classification/{tool}/default/{sample}_{run}.catbat.bins",tool=TOOLS, run=RUNS, sample=SAMPLES, path$[27;1Hrule diamond_db:[28;5Hinput:[29dfaa="/mnt/fass1/kirsten/databases/diamond_all/faa/full_proteome_bacteria_scerevisiae_cneoformans.faa",[30;9Hmap="/mnt/fass1/genomes/new_bacteria/bacteria_blast_db/prot_accession2taxid.txt",[31;9Hnodes="/mnt/fass1/kirsten/databases/diamond/nodes.dmp",[32;9Hnames="/mnt/fass1/kirsten/databases/diamond/names.dmp"[33;5Houtput:[35d(B[0;7m^G(B[m Get Help     (B[0;7m^O(B[m Write Out    (B[0;7m^W(B[m Where Is     (B[0;7m^K(B[m Cut Text     (B[0;7m^J(B[m Justify[81G(B[0;7m^C(B[m Cur Pos[97G(B[0;7m^Y(B[m Prev Page    (B[0;7mM-\(B[m First Line[36d(B[0;7m^X(B[m Exit[36;17H(B[0;7m^R(B[m Read File    (B[0;7m^\(B[m Replace[49G(B[0;7m^U(B[m Uncut Text   (B[0;7m^T(B[m To Spell     (B[0;7m^_(B[m Go To Line   (B[0;7m^V(B[m Next Page    (B[0;7mM-/(B[m Last Line[34d[?12l[?25h[3d[4d[3;8H[1K names="/mnt/fass1/kirsten/databases/diamond/names.dmp"[K[4;5Houtput:[5;8H[1K "/mnt/fass1/kirsten/databases/diamond_all/nr.dmnd"[6;5Hbenchmark:[7;8H[1K "/mnt/fass1/kirsten/result/classification/benchmarks/diamond.db.benchmark.txt"[8d    conda:[K[9;8H[1K "envs/diamond.yaml"[10d    params:[K[11;8H[1K "/mnt/fass1/kirsten/databases/diamond_all/nr"[K[12d    threads: 8[K[13;5Hshell:[14;8H[1K "diamond makedb --in {input.faa} -d {params} --taxonmap {input.map} --taxonnodes {input.nodes} --taxonnames {input.names}"[16drule centrifuge_db:[17d    input:[18;5H    map = "/mnt/fass1/kirsten/databases/centrifuge_all/seqid2taxid.map",[19;8H[1K nodes = "/mnt/fass1/kirsten/databases/centrifuge_all/taxonomy/nodes.dmp",[20;9Hnames ="/mnt/fass1/kirsten/databases/centrifuge_all/taxonomy/names.dmp",[K[21;8H[1K faa="/mnt/fass1/kirsten/databases/centrifuge_all/fna/full_bacteria_scerevisiae_cneoformans.fna"[22d    output:[K[23;7H[1K file1= "/mnt/fass1/kirsten/databases/centrifuge_all/bac_cer_neo.1.cf",[K[24;7H[1K file2= "/mnt/fass1/kirsten/databases/centrifuge_all/bac_cer_neo.2.cf",[K[25;7H[1K file3= "/mnt/fass1/kirsten/databases/centrifuge_all/bac_cer_neo.3.cf"[K[26;5Hthreads: 8[27d    benchmark:[K[28;5H    "/mnt/fass1/kirsten/result/classification/benchmarks/centrifuge.db.benchmark.txt"[29;5Hparams:[K[30;9H"/mnt/fass1/kirsten/databases/centrifuge_all/bar_cer_neo"[K[31;5Hconda:[K[32d"envs/centrifuge.yaml"[K[33;5Hshell:[K[3d[14;33r[14;1H[9T[1;36r[3;9H"envs/centrifuge.yaml"[K[4;5Hshell:[K[5dcentrifuge-build -p {threads} --conversion-table {input.map} --taxonomy-tree {input.nodes} --name-table {input.names} {in$[6;1H[K[7d# creating the project structure[K[8drule create:[9;5Hshell:[K[10;5H    'python structure.py'[11d[K[12ddef get_run(wildcards): #returns the current value of variable/wildcard run[13;5Hreturn wildcards.run[15ddef get_tool(wildcards): #returns the current value of variable/wildcard run[16;5Hreturn wildcards.tool[18ddef get_medianHitLength(wildcards):[19;5Hreturn scripts.getting.get_seqLength(str(PATH[0])+"/data/"+str(wildcards.sample)+".fastq")#/2[20d #   elif str(wildcards.tool) == "kaiju":[21;3H#[10Glengths_default = {'gridion364':201, 'gridion366':194}[22;4H#     return lengths_default[str(wildcards.sample)]/2[23;5H# return wildcards.sample # scripts.getting.get_seqLength({PATH}+str(wildcards.sample)+".fastq")/2[K[25;16H:[K[27;9Hfastq = "{PATH}/data/{sample}.fastq"[K[28;5Houtput:[K[29;9Hfiles = "{PATH}/result/classification/centrifuge/{run}/{sample}_{run}.centrifuge.classification",[30;9Hreport= "{PATH}/result/classification/centrifuge/{run}/{sample}_{run}.centrifuge.report"[K[31;5Hbenchmark:[32;8H "{PATH}/result/classification/benchmarks/{run}/{sample}_{run}.centrifuge.benchmark.txt"[33;5Hthreads: 8[K[3d[23;31r[23;1H[3T[1;36r[3;10H{PATH}/result/classification/benchmarks/{run}/{sample}_{run}.centrifuge.benchmark.txt"[4;5Hthreads: 8[5;5Hparams:[K[6;9Hrunid=get_run,[7;8H[1K db = DI["centrifuge"],  [8d#        medianlength=get_medianHitLength[9;5Hconda[10d"envs/centrifuge.yaml"[11;5Hrun:[12d[1K # -q[41m                            [49m(B[mfiles are fastq[K[13d[41m        [49m(B[m# -x[41m                            [49m(B[mindex files[14d[41m        [49m(B[m# --report-file[41m         [49m(B[mgenerated report file[15d[41m        [49m(B[m# -S[41m                            [49m(B[moutput file[K[16;5H    # -f[16X[16;29Hquery input files are (multi)fasta[17;9H# --ignore-quals[18d[K[19;5H    if 'default' in {params.runid}:[K[20;12H[1K shell('centrifuge -q -x {params.db} {input.fastq} --report-file {output.report} -S {output.files} --ignore-quals')[21;8H[1K elif 'quals' in {params.runid}:[K[22;12H[1K shell('centrifuge -q -x {params.db} {input.fastq} --report-file {output.report} -S {output.files}')[23;9Helif 'medianHitLength' in {params.runid}:[24;14Hshell('centrifuge -q -x {params.db} {input.fastq} --report-file {output.report} -S {output.files} --ignore-quals --mi$[25;9Helse:[26d[1K print("Centrifuge -- Nothing to be done here:", {params.runid})[K[28;6Hkraken2:[K[30;9Hdb = DI['kraken2'],[K[31;5H    files = "{PATH}/data/{sample}.fastq"[32;5Houtput:[K[33;5H    files = "{PATH}/result/classification/kraken2/{run}/{sample}_{run}.kraken2.classification",[3d[25;33r[25;1H[4T[1;36r[18;27r[18;1H[4T[1;36r[2;16r[2;1H[5T[1;36r[3;5Houtput:[4;9Hfiles = "{PATH}/result/classification/kraken2/{run}/{sample}_{run}.kraken2.classification",[5;9Hreport= "{PATH}/result/classification/kraken2/{run}/{sample}_{run}.kraken2.report",[6;9Hunclassified="{PATH}/result/classification/kraken2/{run}/{sample}_{run}.kraken2.unclassified"[7;5Hbenchmark:[8;71Hkraken2[3P[11;22H [12;5Hlog:[K[13d[1K '{PATH}/result/classification/kraken2/{run}/{sample}_{run}.kraken2.log'[15;8H 'envs/main.yaml'[K[17;13Hconfidence  [17;33Hthreshold that must be in [0,1][18;9H# --unclassified-out    prints unclassified sequences to filename[19;9H# --classified-out[33Gprints classified sequences to filename[20;9H# --output[20;33Hprints output to filename[21;9H# --report[21;33Hprints report with aggregate counts/clade to file[24;20Hkraken2 --db {input.db} --unclassified-out {output.unclassified} --report {output.report} --threads {threads} -$[25;15Hmedium' in {params.runid}:#confidence set[26;20Hkraken2 --db {input.db} --report {output.report} --confidence 0.05 --threads {threads} --output {output.files} $[27;15Hrestrictive[4P[28;13Hprint("Sure")[30;20HKraken2 -- Nothing to do[8P[32;7Haiju:[K[3d[22;33r[22;1H[4T[1;36r[12;21r[12;1H[4T[1;36r[6;14r[6;1H[3T[1;36r[3;1Hrule kaiju[4;5Hinput:[K[5ddb = DI['kaiju']+"/kaiju_db_refseq.fmi",[K[6;9Hnodes = DI['kaiju']+"/nodes.dmp",[7;9Hfiles = "{PATH}/data/{sample}.fastq"[8;5Houtput:[9;9Hfiles = '{PATH}/result/classification/kaiju/{run}/{sample}_{run}.kaiju.classification'[K[11;9H'[11;72Haiju.benchmark.txt'  [12;14H4[14;22H,[15;9HmedianHitLength=get_medianHitLength[17;40Haiju/{run}/{sample}_{run}.kaiju[4P[21;12Ht    name of nodes.dmp file[K[22;9H# -f    name of database (.fmi) file[23;9H# -i    input file containing fasta/fastq[24;9H# -o    name of output file[25;9H# -m    minimum match length (default: 11)[26;9H# -E    minimum e-value in Greedy mode (which is default)[28;21Haiju -t {input.nodes} -f {input.db} -i {input.files} -o {output.files} -z {threads} -v')[K[29;19HanHitLength' in {params.runid}:[K[30;21Haiju -t {input.nodes} -f {input.db} -i {input.files} -o {output.files} -z {threads} -v -m {params.medianHitLen[32;14Hass[K[3d[12;21r[21;1H[4S[1;36r[3;12H[1K pass[4;5H    else:[5;9H    print("Kaiju -- Nothing to do here:", {params.runid})[6d[K[7drule kaiju_summary:[K[8;5Hin[1P[9d  nodes = DI['kaiju']+"/nodes.dmp",[K[10;5H    names= DI['kaiju']+"/names.dmp",[11;9Hfiles = "{PATH}/result/classification/kaiju/{run}/{sample}_{run}.kaiju.classification"[12;5Houtput:[13;9H"[13;72Hreport"[16;5Hshell:[17d"kaiju2table -t {input.nodes} -n {input.names} -r species -o {output} {input.files}"[19drule taxmaps: # many folders, fix output[20;5Hinput:[21ddb = DI['taxmaps']+"/refseq_complete_bacarchvir.lcak300.gem",[22;9Htaxonomy = DI['taxmaps']+"/taxonomy.tbl.gz",[23;9Hnodes = DI['kaiju']+"/nodes.dmp",[K[24;9Hfiles = "{PATH}/data/{sample}.fastq"[25;5Hbenchmark:[K[26;9H'{PATH}/result/classification/benchmarks/{run}/{sample}_{run}.taxmaps.benchmark.txt'[27;5Houtput:[K[28;9Ho = '{PATH}/result/classification/taxmaps/{run}/{sample}_{run}.taxmaps.classification',[K[29;9Hfolder = '{PATH}/result/classification/taxmaps/{run}/{sample}'[30;5Hthreads: 8[K[31;5Hparams:[K[32;9Hrunid=get_run,[33;9Hprefix = "{sample}_{run}.taxmaps.classification"[3d[3;9Hrunid=get_run,[4;9Hprefix = "{sample}_{run}.taxmaps.classification"[5;5Hlog:[K[6d"{PATH}/result/classification/taxmaps/{run}/{sample}_{run}.taxmaps.log"[7d    conda:[K[8;5H    'envs/taxmaps.yaml'[9;5Hrun:[K[10d# -f        input fastq[K[11;9H# -l        in preprocessing: minimum read length for mapping[K[12;5H    # -C[12;21Hin preprocessing: entropy cutoff for low complexity filtering[13;9H# -d        index files[K[14;5H    # -t[14;21Htaxonomic rable[15;9H# --cov     coverage histogram[16;5H    # -o[16;21Houtput directory[17;9Hif 'default' in {params.runid}:[K[18;13Hshell('export PERL5LIB=/home/re85gih/miniconda3/envs/taxmaps/opt/krona/lib/'),[19;12H[1K shell('export PATH=$PATH:/home/re85gih/projectClassification/taxmaps/'),[20;12H[1K shell('taxMaps -f {input.files} -c {threads} -t {input.taxonomy} -d {input.db} -o {output.folder} -p {params.prefix}')[21;9Helif 'medium' in {params.runid}:[K[22;9H    pass[K[23;9Helif 'restrictive' in {params.runid}:[24;9H    pass[K[25;5H    else:[K[26;9H    print("TaxMaps -- Nothing to do here:", {params.runid})[K[27d[K[28drule deepmicrobes:[K[29;5Hinput:[K[30;5H    files = "{PATH}/data/{sample}.fastq",[31;5H    kmers =DI["deepmicrobes"]+"/tokens_merged_12mers.txt",[32;9Hweights=DI["deepmicrobes"]+"/weights_species",[33;9Hname2label=DI["deepmicrobes"]+"/name2label_species.txt"[3d[6;13r[6;1H[3T[1;36r[3;9Hweights=DI["deepmicrobes"]+"/weights_species",[4;9Hname2label=DI["deepmicrobes"]+"/name2label_species.txt"[5;5Houtput:[6;9Hprediction="{PATH}/result/classification/deepmicrobes/{run}/{sample}_{run}.deepmicrobes.prediction.tfrec",[7;9Htfrec="{PATH}/result/classification/deepmicrobes/{run}/{sample}_{run}.deepmicrobes.training.tfrec",[8;9Hclassification="{PATH}/result/classification/deepmicrobes/{run}/{sample}_{run}.deepmicrobes.classification",[9;9Hreport="{PATH}/result/classification/deepmicrobes/{run}/{sample}_{run}.deepmicrobes.report"[11;15Hdeepmicrobes.yaml'[12;5Hbenchmark:[13;9H"{PATH}/result/classification/benchmarks/{run}/{sample}_{run}.deepmicrobes.benchmark.txt"[14;5Hthreads: 8[K[15;5Hrun:[K[16;12H-kmer        length of k-mers (default: 12) --> if i want to change that i might need to build my own index[17;9H# --max_len     max length of sequences (default: 150)[18;9H# --pred_out    path to prediction output[K[19;9H# -dd           location of input data[K[20;9H# -ebe          number of training epochs to run between evaluations[K[21;9H# just for me now[K[22d[K[23;9Hshell('export PATH=/home/re85gih/projectClassification/DeepMicrobes/pipelines:$PATH'),[24;9Hshell('export PATH=/home/re85gih/projectClassification/DeepMicrobes/scripts:$PATH'),[25;9Hshell('export PATH=/home/re85gih/projectClassification/DeepMicrobes:$PATH'),[26d[K[27;9H# transform training fastq to tfrec[28;8H[1K #shell('tfrec_train_kmer.sh -i {input.files} -v {input.kmers} -o {output.tfrec}'),[29d[K[30;9H# transform prediction fastq to tfrec[31;9Hshell('tfrec_predict_kmer.sh -f {input.files} -t fastq -v {input.kmers} -o {output.prediction}'),[32d[K[33;9H# make prediction on metagenome datasaet[K[3d[9;19r[9;1H[4T[1;36r[3;1H[K[4;9H# make prediction on metagenome datasaet[K[5;5H    shell('predict_DeepMicrobes.sh -i {output.prediction} -l species -p 8 -m {input.weights} -o {output.classification}'),[6d[K[7;9H# generate taxonomic profiles[K[8;9Hshell('report_profile.sh -i {output.classification} -o {output.report} -t 50 -l {input.name2label}')[K[10drule kslam:[11;5Hinput:[12ddb = DI['kslam'],[13;9Hfiles = "{PATH}/data/{sample}.fastq"[K[14;5Houtput:[15;9H"{PATH}/result/classification/kslam/{run}/{sample}_{run}.kslam.classification"[17;71Hkslam[7P[18;14H16[19;5Hparams:[20;9Hrunid=get_run[K[21;5Hlog:[K[22d"{PATH}/result/classification/kslam/{run}/{sample}_{run}.kslam.log"[23;5Hconda:[K[24d'envs/kslam.yaml'[K[25;5Hrun:[K[26d# --db[26;37Hdatabase file[27;11H--min-alignment-score     alignment score cutoff[28;9Hif 'default' in {params.runid}:[K[29;13Hshell('SLAM --db={input.db} --output-file={output} --num-reads-at-once 1000000 {input.files}')[30;9Helif 'medium' in {params.runid}:[K[31;9H    pass[K[32;9Helif 'restrictive' in {params.runid}:[33;9H    pass[K[3d[7;12r[12;1H[2S[1;36r[11;15r[15;1H
[1;36r[28;33r[28;1H[3T[1;36r[20;29r[20;1HM[1;36r[3;9Helif 'restrictive' in {params.runid}:[4;9H    pass[K[5;9Helse:[K[6dprint("KSLAM -- Nothing to do here:", {params.runid})[8;6Hclark: #output is csv, watch out[10;9H#db = DI['clark']+"/",[11;9Hfiles = "{PATH}/data/{sample}.fastq",[12;9Htargets = "/mnt/fass1/database/clark_database/targets.txt"[14;9Hhelper = "{PATH}/result/classification/clark/{run}/{sample}_{run}.clark.classification.csv",[15;9Hfiles= "{PATH}/result/classification/clark/{run}/{sample}_{run}.clark.classification"[17;71Hclark[18;14H8 [20;13Hdb = "/mnt/fass1/kirsten/database/clark/",[21d[21;9H    runid=get_run[23;39Hclark[23;66Hclark[25;15Hmain[1P[27;12Hk  [21Gk-mer size, has to be between 2 and 32, default:31[28;13Hlong    for long reads (only for full mode)   [29;9H# -m[29;21Hmode of execution[32;20HCLARK --long -O {input.files} -R {output.files} -D {params.db} -n {threads} -T {input.targets}')[3d[17;27r[27;1H[5S[1;36r[2;16r[2;1H[3T[1;36r[3;13Hshell('CLARK --long -O {input.files} -R {output.files} -D {params.db} -n {threads} -T {input.targets}')[4;9Helif 'medium' in {params.runid}:[5;13Hpass[9;20HCLARK[11;11H_abundance:[K[13;9Hres="{PATH}/result/classification/clark/{run}/{sample}_{run}.clark.classification.csv"[14;5Houtput:[K[15;9H"{PATH}/result/classification/clark/{run}/{sample}_{run}.clark.report"[16;5Hparams[17;5H    db = DI['clark']+"/",[18;9Hunnamed="{PATH}/result/classification/clark/{run}/{sample}_{run}.clark.classification"[22;9Hshell("{PATH}/result/classificationBenchmark/scripts/clark.estimate_abundance.sh -F {input.res} -D {params.db} > {output}"$[23;9Hshell("mv {input.res} {params.unnamed}")[25d# preprocessing for ccmetagen[26drule kma:[27;5Hinput:[28ddb = DI['ccmetagen']+"compress_ncbi_nt/ncbi_nt",[29;9Hfiles = "{PATH}/data/{sample}.fastq"[30;5Houtput:[31;9Hresultat="{PATH}/result/classification/ccmetagen/{sample}.kma.intermediate.fortime.res",[32;9Hmapstat="{PATH}/result/classification/ccmetagen/{sample}.kma.intermediate.fortime.mapstat"[K[33;5Hbenchmark:[K[3d[6;21r[21;1H[8S[1;36r[3;33r[3;1H[16T[1;36r[3;5Hbenchmark:[4;9H"{PATH}/result/classification/benchmarks/{run}/{sample}_{run}.clark.benchmark.txt"[5;5Hthreads: 8[6;5Hparams:[7d db = "/mnt/fass1/kirsten/database/clark/",[8;13Hrunid=get_run[9;5Hlog:[10d"{PATH}/result/classification/clark/{run}/{sample}_{run}.clark.log"[11;5Hconda:[12d'envs/main.yaml'[13;5Hrun:[14d# -k[14;21Hk-mer size, has to be between 2 and 32, default:31[15;9H# --long    for long reads (only for full mode)[16;9H# -m[16;21Hmode of execution[18;9Hif 'default' in {params.runid}:[22;5H    elif 'restrictive' in {params.runid}:[23;9H    pass[K[24;5H    else:[25;9H    print("CLARK -- Nothing to do here:", {params.runid})[26d[K[27drule clark_abundance:[28;5Hinput:[K[29;5H    res="{PATH}/result/classification/clark/{run}/{sample}_{run}.clark.classification.csv"[30;5Houtput:[31;9H"{PATH}/result/classification/clark/{run}/{sample}_{run}.clark.report"[32;5Hparams:[33;9Hdb = DI['clark']+"/",[18d[19d[A[A[A[A[A[A[A[A[A[A[A[34d[K[7d            db = "/mnt/fass1/kirsten/database/clar[1;122H(B[0;7mModified[7;46H(B[ms/clark/",[7;47H[34d(B[0;7mSave modified buffer?  (Answering "No" will DISCARD changes.)                                                                      [35;1H Y(B[m Yes[K[36d(B[0;7m N(B[m No  [36;18H(B[0;7mC(B[m Cancel[K[34;63H[35d(B[0;7m^G(B[m Get Help[35;33H(B[0;7mM-D(B[m DOS Format[35;65H(B[0;7mM-A(B[m Append[35;97H(B[0;7mM-B(B[m Backup File[36d(B[0;7m^C(B[m Cancel[17G         [33G(B[0;7mM-M(B[m Mac Format[36;65H(B[0;7mM-P(B[m Prepend[36;97H(B[0;7m^T(B[m To Files[34d(B[0;7mFile Name to Write: Snakefile                                (B[m[34;30H[?25l[34;56H[1K (B[0;7m[ Wrote 527 lines ](B[m[K[J[1;122H(B[0;7m        [36;131H(B[m[36;1H[?12l[?25h[?1049l[?1l>(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:13:57 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0mnano Snakefile snakemake --use-conda -j 8
[33mProvided cores: 8[0m
[33mRules claiming more threads will be scaled down.[0m
[33mJob counts:
	count	jobs
	1	all
	1	clark
	2[0m
[32m[0m
[32mrule clark:
    input: /mnt/fass1/kirsten/data/gridion366.fastq, /mnt/fass1/database/clark_database/targets.txt
    output: /mnt/fass1/kirsten/result/classification/clark/default/gridion366_default.clark.classification.csv, /mnt/fass1/kirsten/result/classification/clark/default/gridion366_default.clark.classification
    log: /mnt/fass1/kirsten/result/classification/clark/default/gridion366_default.clark.log
    jobid: 1
    benchmark: /mnt/fass1/kirsten/result/classification/benchmarks/default/gridion366_default.clark.benchmark.txt
    wildcards: run=default, PATH=/mnt/fass1/kirsten, sample=gridion366
    threads: 8[0m
[32m[0m
CLARK version 1.2.5 (UCR CS&E. Copyright 2013-2018 Rachid Ounit, rouni001@cs.ucr.edu) 
Loading database [/mnt/fass1/kirsten/databases/clark/db_central_k31_t15625_s1610612741_m0.tsk.*] ...
Loading done (database size: 131353 MB read, with sampling factor 2)
Mode: Default,	Processing file: /mnt/fass1/kirsten/data/gridion366.fastq,	 using 8 CPU.
 - Assignment time: 1945.03 s. Speed: 113134 objects/min. (3667480 objects).
 - Results stored in /mnt/fass1/kirsten/result/classification/clark/default/gridion366_default.clark.classification.csv
[31mError in job clark while creating output files /mnt/fass1/kirsten/result/classification/clark/default/gridion366_default.clark.classification, /mnt/fass1/kirsten/result/classification/clark/default/gridion366_default.clark.classification.csv.[0m
[31mMissingOutputException in line 296 of /data/fass1/kirsten/result/classificationBenchmark/Snakefile:
Missing files after 5 seconds:
/mnt/fass1/kirsten/result/classification/clark/default/gridion366_default.clark.classification
This might be due to filesystem latency. If that is the case, consider to increase the wait time with --latency-wait.[0m
[31mExiting because a job execution failed. Look above for error message[0m
[33mWill exit after finishing currently running jobs.[0m
[31mExiting because a job execution failed. Look above for error message[0m
(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:54:29 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:54:29 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:54:29 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:54:29 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:54:29 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:54:29 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:54:29 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:54:29 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:54:29 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:54:29 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:54:29 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:54:29 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:54:29 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:54:29 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:54:29 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:54:29 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:54:29 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:54:29 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:54:29 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:54:29 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:54:29 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:54:29 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:54:29 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:54:29 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:54:29 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:54:29 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:54:29 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:54:29 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:54:29 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:54:29 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:54:29 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:54:29 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:54:29 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:54:29 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:54:29 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:54:29 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:54:29 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:54:29 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:54:29 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:54:29 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:54:29 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:54:29 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:54:29 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:54:29 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:54:29 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:54:29 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:54:29 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:54:29 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:54:29 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:54:29 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:54:29 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:54:29 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:54:29 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:54:29 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:54:29 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:54:29 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:54:29 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:54:29 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:54:29 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:54:29 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:54:29 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:54:29 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:54:29 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:54:29 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:54:29 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:54:29 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:54:29 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:54:29 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:54:29 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:54:29 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:54:29 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:54:29 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:54:29 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:54:29 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0msnakemake --use-conda -j 8[K[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:54:29 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:54:29 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:54:29 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:54:29 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:54:29 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:54:29 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:54:29 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:54:29 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:54:29 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:54:29 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:54:29 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:54:29 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:54:29 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:54:29 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:54:29 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:54:29 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:54:29 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:54:29 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:54:29 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:54:29 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:54:29 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:54:29 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:54:29 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:54:29 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:54:29 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:54:29 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:54:29 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[K(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m10:54:29 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0mnano Snakefile 
[?1049h[1;39r(B[m[4l[?7h[?1h=[?1h=[?1h=[39;49m[?25l[39;49m(B[m[H[2J[37;74H(B[0;7m[ Reading File ](B[m[37;73H(B[0;7m[ Read 527 lines ](B[m[H[39;49m(B[0;7m  GNU nano 2.7.4                                                             File: Snakefile                                                                       [3;1H(B[m# Snakefile for project work of benchmarking different classification tools regarding their usability for long reads[5dconfigfile: "config.yaml"[7d########## VARIABLE DEFINITION[8dDI= dict(config["dataIndex"])[9dPATH = config["path"][10dSAMPLES = "gridion366" #list(config["samples"])[11dTOOLS= 'clark'#ccmetagen centrifuge kraken2 clark kaiju'.split(" ") #list(config["classification"])[12dRUNS='default'#'medianHitLength'#'quals'# 'default medium restrictive'.split()[14dimport scripts.getting[17drule all:[18;5Hinput:[19d# CLASSIFICATION[20;9Hexpand("{path}/result/classification/{tool}/{run}/{sample}_{run}.{tool}.classification", run=RUNS, sample=SAMPLES, tool=TOOLS, path=PATH),[21d# GENERATING (comparable) REPORTS[22d#[9Gexpand("{path}/result/classification/{tool}/{run}/{sample}_{run}.{tool}.areport", run=RUNS, sample=SAMPLES, tool=TOOLS, path=PATH),[23d#[23;10Hexpand("{path}/result/classification/{tool}/{run}/{sample}_{run}.{tool}.report", run=RUNS, sample=SAMPLES, tool=TOOLS, path=PATH),[24d#[9Gexpand("{path}/result/classification/ccmetagen/{sample}.{tool}.intermediate.res", run=RUNS, sample=SAMPLES, tool=TOOLS, path=PATH)[25d#[9Gexpand("{path}/result/classification/{tool}/default/{sample}_{run}.catbat.bins",tool=TOOLS, run=RUNS, sample=SAMPLES, path=PATH)[27drule diamond_db:[28;5Hinput:[29dfaa="/mnt/fass1/kirsten/databases/diamond_all/faa/full_proteome_bacteria_scerevisiae_cneoformans.faa",[30;9Hmap="/mnt/fass1/genomes/new_bacteria/bacteria_blast_db/prot_accession2taxid.txt",[31;9Hnodes="/mnt/fass1/kirsten/databases/diamond/nodes.dmp",[32;9Hnames="/mnt/fass1/kirsten/databases/diamond/names.dmp"[33;5Houtput:[34;9H"/mnt/fass1/kirsten/databases/diamond_all/nr.dmnd"[35;5Hbenchmark:[36;9H"/mnt/fass1/kirsten/result/classification/benchmarks/diamond.db.benchmark.txt"[38d(B[0;7m^G(B[m Get Help     (B[0;7m^O(B[m Write Out    (B[0;7m^W(B[m Where Is     (B[0;7m^K(B[m Cut Text     (B[0;7m^J(B[m Justify[81G(B[0;7m^C(B[m Cur Pos[97G(B[0;7m^Y(B[m Prev Page    (B[0;7mM-\(B[m First Line  (B[0;7mM-W(B[m WhereIs Next(B[0;7m^^(B[m Mark Text[39d(B[0;7m^X(B[m Exit[39;17H(B[0;7m^R(B[m Read File    (B[0;7m^\(B[m Replace[49G(B[0;7m^U(B[m Uncut Text   (B[0;7m^T(B[m To Spell     (B[0;7m^_(B[m Go To Line   (B[0;7m^V(B[m Next Page    (B[0;7mM-/(B[m Last Line   (B[0;7mM-](B[m To Bracket  (B[0;7mM-^(B[m Copy Text[37d[?12l[?25h[3d[4d[5d[6d[7d[8d[9d[10d[11d[12d[13d[14d[15d[16d[17d[18d[19d[20d[21d[22d[23d[A[A[A[A#[37d[K[19d# [20d      expand("{path}/result/classification/{tool}/{run}/{sample}_{run}.{tool}.classification"[1;154H(B[0;7mModified[20;95H(B[m.", run=RUNS, sample=SAMPLES, tool=TOOLS, path=PATH),[20;96Hc", run=RUNS, sample=SAMPLES, tool=TOOLS, path=PATH),[20;97Hs", run=RUNS, sample=SAMPLES, tool=TOOLS, path=PATH),[20;98Hv", run=RUNS, sample=SAMPLES, tool=TOOLS, path=PATH),[20;99H[21;34H[22;99H[23d[24d[25d[26d[27;17H[28;12H[29;99H[30;90H[31;64H[32d[33;12H[34;59H[35;15H[36;87H7[3;37r8[37d[18S[1;39r[19;5Hconda:[20d"envs/diamond.yaml"[21;5Hparams:[22;9H"/mnt/fass1/kirsten/databases/diamond_all/nr"[23;5Hthreads: 8[24;5Hshell:[25d"diamond makedb --in {input.faa} -d {params} --taxonmap {input.map} --taxonnodes {input.nodes} --taxonnames {input.names}"[27drule centrifuge_db:[28;5Hinput:[29dmap = "/mnt/fass1/kirsten/databases/centrifuge_all/seqid2taxid.map",[30;9Hnodes = "/mnt/fass1/kirsten/databases/centrifuge_all/taxonomy/nodes.dmp",[31;9Hnames ="/mnt/fass1/kirsten/databases/centrifuge_all/taxonomy/names.dmp",[32;9Hfaa="/mnt/fass1/kirsten/databases/centrifuge_all/fna/full_bacteria_scerevisiae_cneoformans.fna"[33;5Houtput:[34;8Hfile1= "/mnt/fass1/kirsten/databases/centrifuge_all/bac_cer_neo.1.cf",[35;8Hfile2= "/mnt/fass1/kirsten/databases/centrifuge_all/bac_cer_neo.2.cf",[36;8Hfile3= "/mnt/fass1/kirsten/databases/centrifuge_all/bac_cer_neo.3.cf"[19;11H[20;28H[21;12H[22;54H[23;15H[24;11H[25;99H[26d[27;20H[28;11H[29;77H[30;82H[31d[32;99H[33;12H[34;78H[35d[36d7[3;36r8[18S[1;39r[17;23r[17;1H[3T[1;39r[17;8Hfile2= "/mnt/fass1/kirsten/databases/centrifuge_all/bac_cer_neo.2.cf",[18;8Hfile3= "/mnt/fass1/kirsten/databases/centrifuge_all/bac_cer_neo.3.cf"[19;5Hthreads: 8[20;5Hbenchmark:[K[21;8H "/mnt/fass1/kirsten/result/classification/benchmarks/centrifuge.db.benchmark.txt"[22;5Hparams:[23;9H"/mnt/fass1/kirsten/databases/centrifuge_all/bar_cer_neo"[24;5Hconda:[25d"envs/centrifuge.yaml"[26;5Hshell:[27d"centrifuge-build -p {threads} --conversion-table {input.map} --taxonomy-tree {input.nodes} --name-table {input.names} {input.faa} {params}"[29d# creating the project structure[30drule create:[31;5Hshell:[32d'python structure.py'[34ddef get_run(wildcards): #returns the current value of variable/wildcard run[35;5Hreturn wildcards.run[19;15H[20d[21;90H[22;12H[23;66H[24;11H[25;31H[26;11H[27;99H[28d[29;33H[30;13H[31d[32;30H[33d[34;76H[35;25H[36d[3;5H    "/mnt/fass1/kirsten/result/classification/benchmarks/centrifuge.db.benchmark.txt"[4;5Hparams:[K[5;5H    "/mnt/fass1/kirsten/databases/centrifuge_all/bar_cer_neo"[6;5Hconda[7denvs/centrifuge.yaml"[K[8;5Hshell:[9;8H[1K "centrifuge-build -p {threads} --conversion-table {input.map} --taxonomy-tree {input.nodes} --name-table {input.names} {input.faa} {params}"[10d[K[11d# creating the project structure[K[12drule create:[K[13;5Hshell:[K[14d'python structure.py'[K[15d[K[16ddef get_run(wildcards): #returns the current value of variable/wildcard run  [17;5Hreturn wildcards.run[K[18d[K[19ddef get_tool(wildcards): #returns the current value of variable/wildcard run[20;5Hreturn wildcards.tool[21d[K[22ddef get_medianHitLength(wildcards):[23;5Hreturn scripts.getting.get_seqLength(str(PATH[0])+"/data/"+str(wildcards.sample)+".fastq")#/2[24d #   elif str(wildcards.tool) == "kaiju":[25;3H#      lengths_default = {'gridion364':201, 'gridion366':194}[26;4H#     return lengths_default[str(wildcards.sample)]/2[27;5H# return wildcards.sample # scripts.getting.get_seqLength({PATH}+str(wildcards.sample)+".fastq")/2[K[29drule centrifuge:[K[30d    input:[K[31;5H    fastq = "{PATH}/data/{sample}.fastq"[32;5Houtput:[K[33;9Hfiles = "{PATH}/result/classification/centrifuge/{run}/{sample}_{run}.centrifuge.classification",[34;8H[1K report= "{PATH}/result/classification/centrifuge/{run}/{sample}_{run}.centrifuge.report"[35;5Hbenchmark:[K[36;9H"{PATH}/result/classification/benchmarks/{run}/{sample}_{run}.centrifuge.benchmark.txt"[19;77H[20;26H[21d[22;36H[23;98H[24;42H[25;64H[26;57H[27;99H[28d[29;17H[30;12H[31;45H[32;12H[33;99H[34d[35;15H[36;96H7[3;37r8[37d[18S[1;39r[19;5Hthreads: 8[20;5Hparams:[21;9Hrunid=get_run,[22;9Hdb = DI["centrifuge"],[23d#[23;10Hmedianlength=get_medianHitLength[24;5Hconda:[25;8H"envs/centrifuge.yaml"[26;5Hrun:[27d# -q[41m                            [49m(B[mfiles are fastq[28d[41m        [49m(B[m# -x[41m                            [49m(B[mindex files[29d[41m        [49m(B[m# --report-file[41m         [49m(B[mgenerated report file[30d[41m        [49m(B[m# -S[41m                            [49m(B[moutput file[31;9H# -f[31;29Hquery input files are (multi)fasta[32;9H# --ignore-quals[34;9Hif 'default' in {params.runid}:[35;13Hshell('centrifuge -q -x {params.db} {input.fastq} --report-file {output.report} -S {output.files} --ignore-quals')[36;9Helif 'quals' in {params.runid}:[19;15H[20;12H[21;23H[22;31H[23;42H[24;11H[25;30H[26;9H[27;56H[28;52H[29dle[30d[31;63H[32;25H[33;9H[34;40H[35;99H[36;41H7[3;37r8[37d[18S[1;39r[19;13Hshell('centrifuge -q -x {params.db} {input.fastq} --report-file {output.report} -S {output.files}')[20;9Helif 'medianHitLength' in {params.runid}:[21;14Hshell('centrifuge -q -x {params.db} {input.fastq} --report-file {output.report} -S {output.files} --ignore-quals --min-hit-length {params.medianlengt$[22;9Helse:[23dprint("Centrifuge -- Nothing to be done here:", {params.runid})[25drule kraken2:[26;5Hinput:[27ddb = DI['kraken2'],[28;9Hfiles = "{PATH}/data/{sample}.fastq"[29;5Houtput:[30;9Hfiles = "{PATH}/result/classification/kraken2/{run}/{sample}_{run}.kraken2.classification",[31;9Hreport= "{PATH}/result/classification/kraken2/{run}/{sample}_{run}.kraken2.report",[32;9Hunclassified="{PATH}/result/classification/kraken2/{run}/{sample}_{run}.kraken2.unclassified"[33;5Hbenchmark:[34;9H"{PATH}/result/classification/benchmarks/{run}/{sample}_{run}.kraken2.benchmark.txt"[35;5Hthreads: 8[36;5Hparams:[19;99H[20;51H[21;99H[22;14H[23;76H[24d[25;14H[26;11H[27;28H[28;45H[29;12H[30;99H[31;92H[32;99H[33;15H[34;93H[35;15H[36;12H7[2;37r8[37d[18S[1;39r[2;1H[K[19;9Hrunid=get_run[20;5Hlog:[21d'{PATH}/result/classification/kraken2/{run}/{sample}_{run}.kraken2.log'[22;5Hconda:[23d'envs/main.yaml'[24;5Hrun:[25d# --confidence[25;33Hthreshold that must be in [0,1][26;9H# --unclassified-out    prints unclassified sequences to filename[27;9H# --classified-out[33Gprints classified sequences to filename[28;9H# --output[28;33Hprints output to filename[29;9H# --report[29;33Hprints report with aggregate counts/clade to file[31;9Hif 'default' in {params.runid}:[32;13Hshell('kraken2 --db {input.db} --unclassified-out {output.unclassified} --report {output.report} --threads {threads} --output {output.files} {input.fi$[33;9Helif 'medium' in {params.runid}:#confidence set[34;13Hshell('kraken2 --db {input.db} --report {output.report} --confidence 0.05 --threads {threads} --output {output.files} {input.files}')[35;9Helif 'restrictive' in {params.runid}:[36;13Hprint("Sure")[19;22H[20;9H[21;80H[22;11H[23;25H[24;9H[25;64H[26;74H[27d[28;58H[29;82H[30;9H[31;40H[32;99H[33;57H[34;99H[35;47H[36;27H7[2;37r8[37d[18S[1;39r[2;1H[K[19;9Helse:[20dprint("Kraken2 -- Nothing to do here:", {params.runid})[22drule kaiju:[23;5Hinput:[24ddb = DI['kaiju']+"/kaiju_db_refseq.fmi",[25;9Hnodes = DI['kaiju']+"/nodes.dmp",[26;9Hfiles = "{PATH}/data/{sample}.fastq"[27;5Houtput:[28;9Hfiles = '{PATH}/result/classification/kaiju/{run}/{sample}_{run}.kaiju.classification'[29;5Hbenchmark:[30;9H'{PATH}/result/classification/benchmarks/{run}/{sample}_{run}.kaiju.benchmark.txt'[31;5Hthreads: 4[32;5Hparams:[33;9Hrunid=get_run,[34;9HmedianHitLength=get_medianHitLength[35;5Hlog:[36d'{PATH}/result/classification/kaiju/{run}/{sample}_{run}.kaiju.log'[19;14H[20;79H[21d[22;12H[23d[24;49H[25;42H[26;45H[27;12H[28;95H[29;15H[30;91H[31;15H[32;12H[33;23H[34;44H[35;9H[36;76H7[3;37r8[37d[18S[1;39r[19;5Hconda:[20d'envs/main.yaml'[21;5Hrun:[22d# -t    name of nodes.dmp file[23;9H# -f    name of database (.fmi) file[24;9H# -i    input file containing fasta/fastq[25;9H# -o    name of output file[26;9H# -m    minimum match length (default: 11)[27;9H# -E    minimum e-value in Greedy mode (which is default)[28;9Hif 'default' in {params.runid}:[29;13Hshell('kaiju -t {input.nodes} -f {input.db} -i {input.files} -o {output.files} -z {threads} -v')[30;9Helif 'medianHitLength' in {params.runid}:[31;13Hshell('kaiju -t {input.nodes} -f {input.db} -i {input.files} -o {output.files} -z {threads} -v -m {params.medianHitLength}')[32;9Helif 'restrictive' in {params.runid}:[33;13Hpass[34;9Helse:[35dprint("Kaiju -- Nothing to do here:", {params.runid})[19;11H[20;25H[21;9H[22;39H[23;45H[24;50H[25;38H[26;51H[27;66H[28;40H[29;99H[30;50H[31;99H[32;46H[33;17H[34;14H[35;66H[36d[13;33r[13;1H[7T[1;39r[3;5Hrun:[4d[1K # -t    name of nodes.dmp file[5;5H    # -f    name of database (.fmi) file[6;9H# -i    input file containing fasta/fastq[7;9H# -o    name of output file[K[8;9H# -m    minimum match length (default: 11)[9;5H    # -E    minimum e-value in Greedy mode (which is default)[10;9Hif 'default' in {params.runid}:[K[11;12H[1K shell('kaiju -t {input.nodes} -f {input.db} -i {input.files} -o {output.files} -z {threads} -v')[12;9Helif 'medianHitLength' in {params.runid}:[K[13;13Hshell('kaiju -t {input.nodes} -f {input.db} -i {input.files} -o {output.files} -z {threads} -v -m {params.medianHitLength}')[14;9Helif 'restrictive' in {params.runid}:[15;13Hpass[16;9Helse:[17dprint("Kaiju -- Nothing to do here:", {params.runid})[19drule kaiju_summary:[20;5Hinput:[K[21;5H    nodes = DI['kaiju']+"/nodes.dmp",[22;9Hnames= DI['kaiju']+"/names.dmp",[23;9Hfiles = "{PATH}/result/classification/kaiju/{run}/{sample}_{run}.kaiju.classification"[24;5Houtput:[25;9H"[25;72Hreport"[28;5Hshell:[29d"kaiju2table -t {input.nodes} -n {input.names} -r species -o {output} {input.files}"[30d[K[31drule taxmaps: # many folders, fix output[K[32;5Hinput:[K[33ddb = DI['taxmaps']+"/refseq_complete_bacarchvir.lcak300.gem",[34;9Htaxonomy = DI['taxmaps']+"/taxonomy.tbl.gz",[35;9Hnodes = DI['kaiju']+"/nodes.dmp",[K[36;9Hfiles = "{PATH}/data/{sample}.fastq"[19;20H[20;11H[21;42H[22d[23;96H[24;12H[25;79H[26;11H[27;25H[28;11H[29;96H[30d[31;41H[32;11H[33;70H[34;53H[35;42H[36;45H7[3;37r8[37d[18S[1;39r[19;5Hbenchmark:[20;9H'{PATH}/result/classification/benchmarks/{run}/{sample}_{run}.taxmaps.benchmark.txt'[21;5Houtput:[22;9Ho = '{PATH}/result/classification/taxmaps/{run}/{sample}_{run}.taxmaps.classification',[23;9Hfolder = '{PATH}/result/classification/taxmaps/{run}/{sample}'[24;5Hthreads: 8[25;5Hparams:[26;9Hrunid=get_run,[27;9Hprefix = "{sample}_{run}.taxmaps.classification"[28;5Hlog:[29d"{PATH}/result/classification/taxmaps/{run}/{sample}_{run}.taxmaps.log"[30;5Hconda:[31d'envs/taxmaps.yaml'[32;5Hrun:[33d# -f[33;21Hinput fastq[34;9H# -l[34;21Hin preprocessing: minimum read length for mapping[35;9H# -C[35;21Hin preprocessing: entropy cutoff for low complexity filtering[36;9H# -d[36;21Hindex files[19;15H[20;93H[21;12H[22;96H[23;72H[24;15H[25;12H[26;23H[27;57H[28;9H[29;80H[30;11H[31;29H[32;9H[33;32H[34;70H[35;82H[36;32H7[3;37r8[37d[18S[1;39r[19;9H# -t[19;21Htaxonomic rable[20;9H# --cov     coverage histogram[21;9H# -o[21;21Houtput directory[22;9Hif 'default' in {params.runid}:[23;13Hshell('export PERL5LIB=/home/re85gih/miniconda3/envs/taxmaps/opt/krona/lib/'),[24;13Hshell('export PATH=$PATH:/home/re85gih/projectClassification/taxmaps/'),[25;13Hshell('taxMaps -f {input.files} -c {threads} -t {input.taxonomy} -d {input.db} -o {output.folder} -p {params.prefix}')[26;9Helif 'medium' in {params.runid}:[27;13Hpass[28;9Helif 'restrictive' in {params.runid}:[29;13Hpass[30;9Helse:[31dprint("TaxMaps -- Nothing to do here:", {params.runid})[33drule deepmicrobes:[34;5Hinput:[35dfiles = "{PATH}/data/{sample}.fastq",[36;9Hkmers =DI["deepmicrobes"]+"/tokens_merged_12mers.txt",[19;36H[20;39H[21d[22;40H[23;91H[24;85H[25;99H[26;41H[27;17H[28;46H[29;20H[30;14H[31;69H[32d[33;20H[34;11H[35;46H[36;63H7[3;37r8[37d[18S[1;39r[19;9Hweights=DI["deepmicrobes"]+"/weights_species",[20;9Hname2label=DI["deepmicrobes"]+"/name2label_species.txt"[21;5Houtput:[22;9Hprediction="{PATH}/result/classification/deepmicrobes/{run}/{sample}_{run}.deepmicrobes.prediction.tfrec",[23;9Htfrec="{PATH}/result/classification/deepmicrobes/{run}/{sample}_{run}.deepmicrobes.training.tfrec",[24;9Hclassification="{PATH}/result/classification/deepmicrobes/{run}/{sample}_{run}.deepmicrobes.classification",[25;9Hreport="{PATH}/result/classification/deepmicrobes/{run}/{sample}_{run}.deepmicrobes.report"[26;5Hconda:[27d'envs/deepmicrobes.yaml'[28;5Hbenchmark:[29;9H"{PATH}/result/classification/benchmarks/{run}/{sample}_{run}.deepmicrobes.benchmark.txt"[30;5Hthreads: 8[31;5Hrun:[32d# --kmer[32;25Hlength of k-mers (default: 12) --> if i want to change that i might need to build my own index[33;9H# --max_len     max length of sequences (default: 150)[34;9H# --pred_out    path to prediction output[35;9H# -dd[35;25Hlocation of input data[36;9H# -ebe[36;25Hnumber of training epochs to run between evaluations[19;55H[20;64H[21;12H[22;99H[23d[24d[25d[26;11H[27;33H[28;15H[29;98H[30;15H[31;9H[32;99H[33;63H[34;50H[35;47H[36;77H7[2;37r8[37d[18S[1;39r[2;1H[K[19;9H# just for me now[21;9Hshell('export PATH=/home/re85gih/projectClassification/DeepMicrobes/pipelines:$PATH'),[22;9Hshell('export PATH=/home/re85gih/projectClassification/DeepMicrobes/scripts:$PATH'),[23;9Hshell('export PATH=/home/re85gih/projectClassification/DeepMicrobes:$PATH'),[25;9H# transform training fastq to tfrec[26;9H#shell('tfrec_train_kmer.sh -i {input.files} -v {input.kmers} -o {output.tfrec}'),[28;9H# transform prediction fastq to tfrec[29;9Hshell('tfrec_predict_kmer.sh -f {input.files} -t fastq -v {input.kmers} -o {output.prediction}'),[31;9H# make prediction on metagenome datasaet[32;9Hshell('predict_DeepMicrobes.sh -i {output.prediction} -l species -p 8 -m {input.weights} -o {output.classification}'),[34;9H# generate taxonomic profiles[35;9Hshell('report_profile.sh -i {output.classification} -o {output.report} -t 50 -l {input.name2label}')[19;26H[20;9H[21;95H[22d[23;85H[24;9H[25;44H[26;91H[27;9H[28;46H[29;99H[30;9H[31;49H[32;99H[33;9H[34;38H[35;99H[36d7[2;37r8[37d[18S[1;39r[19;1Hrule kslam:[20;5Hinput:[21ddb = DI['kslam'],[22;9Hfiles = "{PATH}/data/{sample}.fastq"[23;5Houtput:[24;9H"{PATH}/result/classification/kslam/{run}/{sample}_{run}.kslam.classification"[25;5Hbenchmark:[26;9H"{PATH}/result/classification/benchmarks/{run}/{sample}_{run}.kslam.benchmark.txt"[27;5Hthreads: 16[28;5Hparams:[29;9Hrunid=get_run[30;5Hlog:[31d"{PATH}/result/classification/kslam/{run}/{sample}_{run}.kslam.log"[32;5Hconda:[33d'envs/kslam.yaml'[34;5Hrun:[35d# --db[35;37Hdatabase file[36;9H# --min-alignment-score     alignment score cutoff[19;12H[20d[21;26H[22;45H[23;12H[24;88H[25;15H[26;91H[27;16H[28;12H[29;22H[30;9H[31;76H[32;11H[33;26H[34;9H[35;50H[36;59H7[2;36r8[18S[1;39r[20;36r[20;1H[9T[1;39r[2;1H[K[19;9Hif 'default' in {params.runid}:[20;13Hshell('SLAM --db={input.db} --output-file={output} --num-reads-at-once 1000000 {input.files}')[21;9Helif 'medium' in {params.runid}:[22;13Hpass[23;9Helif 'restrictive' in {params.runid}:[24;13Hpass[25;9Helse:[26dprint("KSLAM -- Nothing to do here:", {params.runid})[28drule clark: #output is csv, watch out[29;5Hinput:[30d#db = DI['clark']+"/",[31;9Hfiles = "{PATH}/data/{sample}.fastq",[32;9Htargets = "/mnt/fass1/database/clark_database/targets.txt"[33;5Houtput:[34;9Hhelper = "{PATH}/result/classification/clark/{run}/{sample}_{run}.clark.classification.csv",[35;9Hfiles= "{PATH}/result/classification/clark/{run}/{sample}_{run}.clark.classification"[36;5Hbenchmark:[19;40H[20;99H[21;41H[22;17H[23;46H[24;17H[25;14H[26;66H[27;9H[28;38H[29;11H[30;31H[31;46H[32;67H[33;12H[34;99H[35;95H[36;15H[3;9Helif 'medium' in {params.runid}:[4;9H    pass[K[5;5H    elif 'restrictive' in {params.runid}:[6;9H    pass[K[7;5H    else:[K[8;9H    print("KSLAM -- Nothing to do here:", {params.runid})[K[9d[K[10drule clark: #output is csv, watch out[11;5Hinput:[K[12;5H    #db = DI['clark']+"/",[13;9Hfiles = "{PATH}/data/{sample}.fastq",[K[14;5H    targets = "/mnt/fass1/database/clark_database/targets.txt"[15;5Houtput:[K[16;5H    helper = "{PATH}/result/classification/clark/{run}/{sample}_{run}.clark.classification.csv",[17;9Hfiles= "{PATH}/result/classification/clark/{run}/{sample}_{run}.clark.classification"[18;5Hbenchmark:[K[19;9H"{PATH}/result/classification/benchmarks/{run}/{sample}_{run}.clark.benchmark.txt"[20;5Hthreads: 8[K[21;5Hparams:[K[22d db = "/mnt/fass1/kirsten/databases/clark/",[23;9H    runid=get_run[K[24;5Hlog:[K[25d"{PATH}/result/classification/clark/{run}/{sample}_{run}.clark.log"[26;5Hconda:[K[27d'envs/main.yaml'[28d    run:[K[29;5H    # -k[29;21Hk-mer size, has to be between 2 and 32, default:31[30;10H --long    for long reads (only for full mode)[31;9H# -m        mode of execution[K[32d[K[33;5H    if 'default' in {params.runid}:[34;9H    shell('CLARK --long -O {input.files} -R {output.files} -D {params.db} -n {threads} -T {input.targets}')[35;9Helif 'medium' in {params.runid}:[K[36;12H[1K pass[19;91H[20;15H[21;12H[22;56H[23;26H[24;9H[25;76H[26;11H[27;25H[28;9H[27;25H[26;11H[25;76H[24;9H[23;26H[22;56H[21;12H[A: 8[19;91H[18;15H[17;95H[A.csv[17;95H[18;15H[19;91H[20;15H[21;12H[22;56H[23;26H[24;9H[25;76H[26;11H[27;25H[28;9H[29;72H[30;56H[31;38H[32d[33;40H[34;99H[35;41H[36;17H[37d(B[0;7mSave modified buffer?  (Answering "No" will DISCARD changes.)                                                                                                      [38;1H Y(B[m Yes[K[39d(B[0;7m N(B[m No  [39;18H(B[0;7mC(B[m Cancel[K[37;63H[38d(B[0;7m^G(B[m Get Help[38;41H(B[0;7mM-D(B[m DOS Format[38;81H(B[0;7mM-A(B[m Append[38;121H(B[0;7mM-B(B[m Backup File[39d(B[0;7m^C(B[m Cancel[17G         [39;41H(B[0;7mM-M(B[m Mac Format[39;81H(B[0;7mM-P(B[m Prepend[39;121H(B[0;7m^T(B[m To Files[37d(B[0;7mFile Name to Write: Snakefile                                (B[m[37;30H[?25l[37;72H[1K (B[0;7m[ Wrote 527 lines ](B[m[K[J[1;154H(B[0;7m        [39;163H(B[m[39;1H[?12l[?25h[?1049l[?1l>(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m11:39:30 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0m[H[2J(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m11:39:30 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0mnano Snakefile snakemake --use-conda -j 8
[33mProvided cores: 8[0m
[33mRules claiming more threads will be scaled down.[0m
[33mJob counts:
	count	jobs
	1	all
	1	clark
	2[0m
[32m[0m
[32mrule clark:
    input: /mnt/fass1/database/clark_database/targets.txt, /mnt/fass1/kirsten/data/gridion366.fastq
    output: /mnt/fass1/kirsten/result/classification/clark/default/gridion366_default.clark.classification.csv, /mnt/fass1/kirsten/result/classification/clark/default/gridion366_default.clark.classification
    log: /mnt/fass1/kirsten/result/classification/clark/default/gridion366_default.clark.log
    jobid: 1
    benchmark: /mnt/fass1/kirsten/result/classification/benchmarks/default/gridion366_default.clark.benchmark.txt
    wildcards: sample=gridion366, run=default, PATH=/mnt/fass1/kirsten
    threads: 8[0m
[32m[0m
CLARK version 1.2.5 (UCR CS&E. Copyright 2013-2018 Rachid Ounit, rouni001@cs.ucr.edu) 
Loading database [/mnt/fass1/kirsten/databases/clark/db_central_k31_t15625_s1610612741_m0.tsk.*] ...
^C[33mTerminating processes on user request.[0m
[31mError in job clark while creating output files /mnt/fass1/kirsten/result/classification/clark/default/gridion366_default.clark.classification.csv, /mnt/fass1/kirsten/result/classification/clark/default/gridion366_default.clark.classification.[0m
[31mRuleException:
CalledProcessError in line 320 of /data/fass1/kirsten/result/classificationBenchmark/Snakefile:
Command 'CLARK --long -O /mnt/fass1/kirsten/data/gridion366.fastq -R /mnt/fass1/kirsten/result/classification/clark/default/gridion366_default.clark.classification -D /mnt/fass1/kirsten/databases/clark/ -n 8 -T /mnt/fass1/database/clark_database/targets.txt' returned non-zero exit status -2
  File "/data/fass1/kirsten/result/classificationBenchmark/Snakefile", line 320, in __rule_clark
  File "/usr/lib/python3.5/concurrent/futures/thread.py", line 55, in run[0m
(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m11:41:14 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0mnano Snakefile +[K[K
[?1049h[1;39r(B[m[4l[?7h[?1h=[?1h=[?1h=[39;49m[?25l[39;49m(B[m[H[2J[37;55H(B[0;7m[ Reading File ](B[m[37;54H(B[0;7m[ Read 527 lines ](B[m[H[39;49m(B[0;7m  GNU nano 2.7.4                                          File: Snakefile                                                    [3;1H(B[m# Snakefile for project work of benchmarking different classification tools regarding their usability for long reads[5dconfigfile: "config.yaml"[7d########## VARIABLE DEFINITION[8dDI= dict(config["dataIndex"])[9dPATH = config["path"][10dSAMPLES = "gridion366" #list(config["samples"])[11dTOOLS= 'clark'#ccmetagen centrifuge kraken2 clark kaiju'.split(" ") #list(config["classification"])[12dRUNS='default'#'medianHitLength'#'quals'# 'default medium restrictive'.split()[14dimport scripts.getting[17drule all:[18;5Hinput:[19d# CLASSIFICATION[20;9Hexpand("{path}/result/classification/{tool}/{run}/{sample}_{run}.{tool}.classification.csv", run=RUNS, sample=SAMPLE$[21;1H# GENERATING (comparable) REPORTS[22d#[9Gexpand("{path}/result/classification/{tool}/{run}/{sample}_{run}.{tool}.areport", run=RUNS, sample=SAMPLES, tool=TOO$[23;1H#[23;10Hexpand("{path}/result/classification/{tool}/{run}/{sample}_{run}.{tool}.report", run=RUNS, sample=SAMPLES, tool=TOO$[24;1H#[9Gexpand("{path}/result/classification/ccmetagen/{sample}.{tool}.intermediate.res", run=RUNS, sample=SAMPLES, tool=TOO$[25;1H#[9Gexpand("{path}/result/classification/{tool}/default/{sample}_{run}.catbat.bins",tool=TOOLS, run=RUNS, sample=SAMPLES$[27;1Hrule diamond_db:[28;5Hinput:[29dfaa="/mnt/fass1/kirsten/databases/diamond_all/faa/full_proteome_bacteria_scerevisiae_cneoformans.faa",[30;9Hmap="/mnt/fass1/genomes/new_bacteria/bacteria_blast_db/prot_accession2taxid.txt",[31;9Hnodes="/mnt/fass1/kirsten/databases/diamond/nodes.dmp",[32;9Hnames="/mnt/fass1/kirsten/databases/diamond/names.dmp"[33;5Houtput:[34;9H"/mnt/fass1/kirsten/databases/diamond_all/nr.dmnd"[35;5Hbenchmark:[36;9H"/mnt/fass1/kirsten/result/classification/benchmarks/diamond.db.benchmark.txt"[38d(B[0;7m^G(B[m Get Help    (B[0;7m^O(B[m Write Out   (B[0;7m^W(B[m Where Is    (B[0;7m^K(B[m Cut Text    (B[0;7m^J(B[m Justify     (B[0;7m^C(B[m Cur Pos     (B[0;7m^Y(B[m Prev Page   (B[0;7mM-\(B[m First Line[39d(B[0;7m^X(B[m Exit[39;16H(B[0;7m^R(B[m Read File   (B[0;7m^\(B[m Replace     (B[0;7m^U(B[m Uncut Text  (B[0;7m^T(B[m To Spell    (B[0;7m^_(B[m Go To Line  (B[0;7m^V(B[m Next Page   (B[0;7mM-/(B[m Last Line[37d[?12l[?25h[3d[4d[5d[6d[7d[8d[9d[10d[11d[12d[13d[14d[15d[16d[17d[18d[19d[20d[9;36r[36;1H[14S[1;39r[3;1H    benchmark:[K[4;9H"/mnt/fass1/kirsten/result/classification/benchmarks/diamond.db.benchmark.txt"[5d    conda:[K[6d"envs/diamond.yaml"[7d    params:[K[8;8H[1K "/mnt/fass1/kirsten/databases/diamond_all/nr"[9d    threads: 8[K[10d    shell:[K[11;8H[1K "diamond makedb --in {input.faa} -d {params} --taxonmap {input.map} --taxonnodes {input.nodes} --taxonnames {input.n[13;6Hcentrifuge_db:[15;9Hmap = "/mnt/fass1/kirsten/databases/centrifuge_all/seqid2taxid.map",[K[16;9Hnodes = "/mnt/fass1/kirsten/databases/centrifuge_all/taxonomy/nodes.dmp",[K[17;10Hames ="/mnt/fass1/kirsten/databases/centrifuge_all/taxonomy/names.dmp",[18;9Hfaa="/mnt/fass1/kirsten/databases/centrifuge_all/fna/full_bacteria_scerevisiae_cneoformans.fna"[20;8Hfile1= "/mnt/fass1/kirsten/databases/centrifuge_all/bac_cer_neo.1.cf",[21;5H   file2= "/mnt/fass1/kirsten/databases/centrifuge_all/bac_cer_neo.2.cf",[22;8Hfile3= "/mnt/fass1/kirsten/databases/centrifuge_all/bac_cer_neo.3.cf"[K[23;5Hthreads: 8[24;5Hbenchmark:[25;9H"/mnt/fass1/kirsten/result/classification/benchmarks/centrifuge.db.benchmark.txt"[26;5Hparams:[27;9H"/mnt/fass1/kirsten/databases/centrifuge_all/bar_cer_neo"[28;5Hconda:[29d"envs/centrifuge.yaml"[30;5Hshell:[31d"centrifuge-build -p {threads} --conversion-table {input.map} --taxonomy-tree {input.nodes} --name-table {input.name$[33;1H# creating the project structure[34drule create:[35;5Hshell:[36d'python structure.py'[3d[18;27r[18;1H[3T[1;39r[3;5Hshell:[K[4d'python structure.py'[K[5d[K[6ddef get_run(wildcards): #returns the current value of variable/wildcard run[7;5Hreturn wildcards.run[8d[K[9ddef get_tool(wildcards): #returns the current value of variable/wildcard run[10;5Hreturn wildcards.tool[11d[K[12ddef get_medianHitLength(wildcards):[13d    return scripts.getting.get_seqLength(str(PATH[0])+"/data/"+str(wildcards.sample)+".fastq")#/2[14d #   elif str(wildcards.tool) == "kaiju":[15;3H#      lengths_default = {'gridion364':201, 'gridion366':194}[K[16;4H#     return lengths_default[str(wildcards.sample)]/2[K[17;5H# return wildcards.sample # scripts.getting.get_seqLength({PATH}+str(wildcards.sample)+".fastq")/2[19drule centrifuge:[20;5Hinput:[21dstq = "{PATH}/data/{sample}.fastq"[K[23;8H files = "{PATH}/result/classification/centrifuge/{run}/{sample}_{run}.centrifuge.classification",[24;8H report= "{PATH}/result/classification/centrifuge/{run}/{sample}_{run}.centrifuge.report"[25;5Hbenchmark:[K[26;5H    "{PATH}/result/classification/benchmarks/{run}/{sample}_{run}.centrifuge.benchmark.txt"[27;5Hthreads: 8[28;5Hparams:[29;9Hrunid=get_run,[K[30;5H    db = DI["centrifuge"],[31d#[9G medianlength=get_medianHitLength[K[32;5Hconda:[33;7H[1K "envs/centrifuge.yaml"   [34d    run:[K[35;5H    # -q[41m                            [49m(B[mfiles are fastq[36d[41m        [49m(B[m# -x[41m                            [49m(B[mindex files[3d[32;36r[32;1H[2T[1;39r[25;31r[25;1H[2T[1;39r[21;25r[21;1HM[1;39r[3;5H    # -q[41m                            [49m(B[mfiles are fastq[4d[41m        [49m(B[m# -x[41m                            [49m(B[mindex files[5d[41m        [49m(B[m# --report-file[41m         [49m(B[mgenerated report file[6d[41m        [49m(B[m# -S[41m                            [49m(B[moutput file[K[7;5H    # -f[16X[7;29Hquery input files are (multi)fasta[8;9H# --ignore-quals[9d[K[10;5H    if 'default' in {params.runid}:[11;13Hshell('centrifuge -q -x {params.db} {input.fastq} --report-file {output.report} -S {output.files} --ignore-quals$[12;8H[1K elif 'quals' in {params.runid}:[13;12H[1K shell('centrifuge -q -x {params.db} {input.fastq} --report-file {output.report} -S {output.files}')[14;8H[1K elif 'medianHitLength' in {params.runid}:[15;13H[1K shell('centrifuge -q -x {params.db} {input.fastq} --report-file {output.report} -S {output.files} --ignore-qual$[16;8H[1K else:[K[17d[1K print("Centrifuge -- Nothing to be done here:", {params.runid})[K[19;6Hkraken2:[K[21;9Hdb = DI['kraken2'],[22;10Hiles[24;47Hkraken2/{run}/{sample}_{run}.kraken2[6P[25;47Hkraken2/{run}/{sample}_{run}.kraken2.report",[K[26;9Hunclassified="{PATH}/result/classification/kraken2/{run}/{sample}_{run}.kraken2.unclassified"[28;71Hkraken2[3P[31;22H [32;5Hlog:[33d'{PATH}/result/classification/kraken2/{run}/{sample}_{run}.kraken2.log'[35;8H 'envs/main.yaml'[K[3d[32;36r[32;1HM[1;39r[22;26r[22;1H[2T[1;39r[16;23r[16;1HM[1;39r[8;15r[8;1HM[1;39r[3;9H'envs/main.yaml'[K[4d    run:[K[5d[1K # --confidence          threshold that must be in [0,1][6;8H[1K # --unclassified-out    prints unclassified sequences to filename[7;12H-classified-out      prints classified sequences to filename[8;9H# --output[8;33Hprints output to filename[9;13Hreport      [9;33Hprints report with aggregate counts/clade to file[12;20Hkraken2 --db {input.db} --unclassified-out {output.unclassified} --report {output.report} --threads {thre[13;15Hmedium' in {params.runid}:#confidence set[14;20Hkraken2 --db {input.db} --report {output.report} --confidence 0.05 --threads {threads} --output {output.f$[15;15Hrestrictive[4P[16;13Hprint("Sure")[18;20HKraken2 -- Nothing to do[8P[20;7Haiju:[K[22;19Haiju']+"/kaiju_db_refseq.fmi",[23;9Hnodes = DI['kaiju']+"/nodes.dmp",[26;17H'[26;48Haiju/{run}/{sample}_{run}.kaiju.classification'[K[28;9H'[28;72Haiju.benchmark.txt'  [29;14H4[31;22H,[32;9HmedianHitLength=get_medianHitLength[34;40Haiju/{run}/{sample}_{run}.kaiju[4P[3d[17;22r[17;1HM[1;39r[13;16r[13;1HM[1;39r[9;12r[9;1HM[1;39r[2;6r[2;1HM[1;39r[3;5Hconda:[6d-t    name of nodes.dmp file[K[7;12Hf    name of database (.fmi) file[K[8;12Hi    input file containing fasta/fastq[K[9;9H# -o    name of output file[10;12Hm    minimum match length (default: 11)[K[11;9H# -E    minimum e-value in Greedy mode (which is default)[13;13Hshell('kaiju -t {input.nodes} -f {input.db} -i {input.files} -o {output.files} -z {threads} -v')[14;19HanHitLength' in {params.runid}:[K[15;21Haiju -t {input.nodes} -f {input.db} -i {input.files} -o {output.files} -z {threads} -v -m {params.median[17;13Hpass[19;21Haiju[2P[21;11H_summary:[24;9Hnames= DI['kaiju']+"/names.dmp",[K[25;5H    files = "{PATH}/result/classification/kaiju/{run}/{sample}_{run}.kaiju.classification"[26;5Houtput:[K[27;5H    "{PATH}/result/classification/kaiju/{run}/{sample}_{run}.kaiju.report"[28;5Hconda:[K[29;5H    'envs/main.yaml'[30;5Hshell:[K[31d"kaiju2table -t {input.nodes} -n {input.names} -r species -o {output} {input.files}"[32d[K[33drule taxmaps: # many folders, fix output[34;5Hinput:[K[35;5H    db = DI['taxmaps']+"/refseq_complete_bacarchvir.lcak300.gem",[36;9Htaxonomy = DI['taxmaps']+"/taxonomy.tbl.gz",[3d        db = DI['taxmaps']+"/refseq_complete_bacarchvir.lcak300.gem",[4;9Htaxonomy = DI['taxmaps']+"/taxonomy.tbl.gz",[5;5H    nodes = DI['kaiju']+"/nodes.dmp",[6;9Hfiles = "{PATH}/data/{sample}.fastq"[7;5Hbenchmark:[K[8;9H'{PATH}/result/classification/benchmarks/{run}/{sample}_{run}.taxmaps.benchmark.txt'[9;5Houtput:[K[10;9Ho = '{PATH}/result/classification/taxmaps/{run}/{sample}_{run}.taxmaps.classification',[11;9Hfolder = '{PATH}/result/classification/taxmaps/{run}/{sample}'[12;5Hthreads: 8[K[13;5Hparams:[K[14;9Hrunid=get_run,[K[15;9Hprefix = "{sample}_{run}.taxmaps.classification"[K[16;5Hlog:[K[17d"{PATH}/result/classification/taxmaps/{run}/{sample}_{run}.taxmaps.log"[18;5Hconda:[K[19d'envs/taxmaps.yaml'[K[20;5Hrun:[21d[1K # -f        input fastq[22;5H    # -l[22;21Hin preprocessing: minimum read length for mapping[23;9H# -C        in preprocessing: entropy cutoff for low complexity filtering[24;9H# -d        index files[K[25;9H# -t        taxonomic rable[K[26;5H    # --cov     coverage histogram[27;9H# -o        output directory[K[28;5H    if 'default' in {params.runid}:[29;9H    shell('export PERL5LIB=/home/re85gih/miniconda3/envs/taxmaps/opt/krona/lib/'),[30;12H[1K shell('export PATH=$PATH:/home/re85gih/projectClassification/taxmaps/'),[31;9H    shell('taxMaps -f {input.files} -c {threads} -t {input.taxonomy} -d {input.db} -o {output.folder} -p {params.pre$[32;9Helif 'medium' in {params.runid}:[33;12H[1K pass[K[34;5H    elif 'restrictive' in {params.runid}:[35;9H    pass[K[36;9Helse:[K[3d[8;16r[8;1H[4T[1;39r[3;9H    pass[K[4;9Helse:[K[5;9H    print("TaxMaps -- Nothing to do here:", {params.runid})[6d[K[7drule deepmicrobes:[8;5Hinput:[9dfiles = "{PATH}/data/{sample}.fastq",[10;9Hkmers =DI["deepmicrobes"]+"/tokens_merged_12mers.txt",[11;9Hweights=DI["deepmicrobes"]+"/weights_species",[12;9Hname2label=DI["deepmicrobes"]+"/name2label_species.txt"[K[14;9Hprediction="{PATH}/result/classification/deepmicrobes/{run}/{sample}_{run}.deepmicrobes.prediction.tfrec",[15;9Htfrec="{PATH}/result/classification/deepmicrobes/{run}/{sample}_{run}.deepmicrobes.training.tfrec",[16;5H    classification="{PATH}/result/classification/deepmicrobes/{run}/{sample}_{run}.deepmicrobes.classification",[17;9Hreport="{PATH}/result/classification/deepmicrobes/{run}/{sample}_{run}.deepmicrobes.report"[19;15Hdeepmicrobes.yaml'[20;5Hbenchmark:[21;9H"{PATH}/result/classification/benchmarks/{run}/{sample}_{run}.deepmicrobes.benchmark.txt"[22;5Hthreads: 8[K[23;5Hrun:[K[24;12H-kmer        length of k-mers (default: 12) --> if i want to change that i might need to build my own index[25;12H-max_len     max length of sequences (default: 150)[26;13Hpred_out    path to prediction output[27;12Hdd[21G    location of input data[28;9H# -ebe          number of training epochs to run between evaluations[29;9H# just for me now[K[30d[K[31;9Hshell('export PATH=/home/re85gih/projectClassification/DeepMicrobes/pipelines:$PATH'),[K[32;9Hshell('export PATH=/home/re85gih/projectClassification/DeepMicrobes/scripts:$PATH'),[33;9Hshell('export PATH=/home/re85gih/projectClassification/DeepMicrobes:$PATH'),[34d[K[35;9H# transform training fastq to tfrec[36;9H#shell('tfrec_train_kmer.sh -i {input.files} -v {input.kmers} -o {output.tfrec}'),[3d[14;24r[14;1HM[1;39r[3;9H# transform training fastq to tfrec[4;9H#shell('tfrec_train_kmer.sh -i {input.files} -v {input.kmers} -o {output.tfrec}'),[5d[K[6;9H# transform prediction fastq to tfrec[7;8H[1K shell('tfrec_predict_kmer.sh -f {input.files} -t fastq -v {input.kmers} -o {output.prediction}'),[8d[K[9;9H# make prediction on metagenome datasaet[10;9Hshell('predict_DeepMicrobes.sh -i {output.prediction} -l species -p 8 -m {input.weights} -o {output.classification}'$[11;1H[K[12;9H# generate taxonomic profiles[K[13;5H    shell('report_profile.sh -i {output.classification} -o {output.report} -t 50 -l {input.name2label}')[15drule kslam:[K[16;5Hinput:[K[17ddb = DI['kslam'],[K[18;9Hfiles = "{PATH}/data/{sample}.fastq"[K[19;5Houtput:[20;9H"{PATH}/result/classification/kslam/{run}/{sample}_{run}.kslam.classification"[22;71Hkslam[7P[23;14H16[24;5Hparams:[25;9Hrunid=get_run[K[26;5Hlog:[K[27d"{PATH}/result/classification/kslam/{run}/{sample}_{run}.kslam.log"[28;5Hconda:[K[29d'envs/kslam.yaml'[30;5Hrun:[31d# --db[22X[31;37Hdatabase file[K[32;9H# --min-alignment-score     alignment score cutoff[K[33;9Hif 'default' in {params.runid}:[K[34;13Hshell('SLAM --db={input.db} --output-file={output} --num-reads-at-once 1000000 {input.files}')[35;9Helif 'medium' in {params.runid}:   [36;9H    pass[K[3d[6;16r[16;1H[5S[1;39r[12;19r[19;1H[4S[1;39r[16;24r[24;1H[3S[1;39r[22;32r[32;1H[2S[1;39r[3;9Helif 'medium' in {params.runid}:   [4;9H    pass[K[5;9Helif 'restrictive' in {params.runid}:[6;13Hpass[7;9Helse:[K[8;9H    print("KSLAM -- Nothing to do here:", {params.runid})[K[10;6Hclark: #output is csv, watch out[12;9H#db = DI['clark']+"/",[13;9Hfiles = "{PATH}/data/{sample}.fastq",[14;9Htargets = "/mnt/fass1/database/clark_database/targets.txt"[16;9Hhelper = "{PATH}/result/classification/clark/{run}/{sample}_{run}.clark.classification.csv",[17;9Hfiles= "{PATH}/result/classification/clark/{run}/{sample}_{run}.clark.classification"[19;71Hclark[20;14H8 [22;13Hdb = "/mnt/fass1/kirsten/databases/clark/",[23d[23;9H    runid=get_run[25;39Hclark[25;66Hclark[27;15Hmain[1P[29;12Hk  [21Gk-mer size, has to be between 2 and 32, default:31[30;13Hlong    for long reads (only for full mode)   [31;9H# -m[31;21Hmode of execution[34;20HCLARK --long -O {input.files} -R {output.files} -D {params.db} -n {threads} -T {input.targets}')[37d[K[3d[4d[5d[6d[7d[8d[9d[10d[11d[12d[13d[14d[15d[16d[17d        [1;116H(B[0;7mModified[17;9H(B[m#files= "{PATH}/result/classification/clark/{run}/{sample}_{run}.clark.classification"[17;10H[18d[19d[20d[21d[22d[23d[24d[25d"[26d[27d[28d[29d#[30d[31d[32d[33;10H[A[31;10H[A[A[A[A'[A[A[A[A    runid=get_run,7[24;36r8[24dM[1;39r[24;1H            files= "{PATH}/result/classification/clark/{run}/{sample}_{run}.clark.classification"[25;9H[26;76H[27;11H[28;25H[29;9H[30;72H[31;56H[32;38H[33d[34;40H[35;98Ht[1P[1P[1P[1P[1P[1Pp.files} -D {params.db} -n {threads} -T {input.targets}')[35;55Ha.files} -D {params.db} -n {threads} -T {input.targets}')[35;56Hr.files} -D {params.db} -n {threads} -T {input.targets}')[35;57Ha.files} -D {params.db} -n {threads} -T {input.targets}')[35;58Hm.files} -D {params.db} -n {threads} -T {input.targets}')[35;59Hs.files} -D {params.db} -n {threads} -T {input.targets}')[35;60H..files} -D {params.db} -n {threads} -T {input.targets}')[35;61H[1P[37d(B[0;7mSave modified buffer?  (Answering "No" will DISCARD changes.)                                                                [38;1H Y(B[m Yes[K[39d(B[0;7m N(B[m No  [39;16H (B[0;7m^C(B[m Cancel[K[37;63H[38d(B[0;7m^G(B[m Get Help[38;32H(B[0;7mM-D(B[m DOS Format[38;63H(B[0;7mM-A(B[m Append[38;94H(B[0;7mM-B(B[m Backup File[39d(B[0;7m^C(B[m Cancel[17G         [32G(B[0;7mM-M(B[m Mac Format[39;63H(B[0;7mM-P(B[m Prepend[39;94H(B[0;7m^T(B[m To Files[37d(B[0;7mFile Name to Write: Snakefile                                (B[m[37;30H[?25l[37;53H[1K (B[0;7m[ Wrote 528 lines ](B[m[K[J[1;116H(B[0;7m        [39;125H(B[m[39;1H[?12l[?25h[?1049l[?1l>(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m11:42:11 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0mnano Snakefilesnakemake --use-conda -j 8
[33mProvided cores: 8[0m
[33mRules claiming more threads will be scaled down.[0m
[33mJob counts:
	count	jobs
	1	all
	1	clark
	2[0m
[32m[0m
[32mrule clark:
    input: /mnt/fass1/kirsten/data/gridion366.fastq, /mnt/fass1/database/clark_database/targets.txt
    output: /mnt/fass1/kirsten/result/classification/clark/default/gridion366_default.clark.classification.csv
    log: /mnt/fass1/kirsten/result/classification/clark/default/gridion366_default.clark.log
    jobid: 1
    benchmark: /mnt/fass1/kirsten/result/classification/benchmarks/default/gridion366_default.clark.benchmark.txt
    wildcards: PATH=/mnt/fass1/kirsten, run=default, sample=gridion366
    threads: 8[0m
[32m[0m
CLARK version 1.2.5 (UCR CS&E. Copyright 2013-2018 Rachid Ounit, rouni001@cs.ucr.edu) 
Loading database [/mnt/fass1/kirsten/databases/clark/db_central_k31_t15625_s1610612741_m0.tsk.*] ...
Loading done (database size: 131353 MB read, with sampling factor 2)
Mode: Default,	Processing file: /mnt/fass1/kirsten/data/gridion366.fastq,	 using 8 CPU.
 - Assignment time: 1992.93 s. Speed: 110414 objects/min. (3667480 objects).
 - Results stored in /mnt/fass1/kirsten/result/classification/clark/default/gridion366_default.clark.classification.csv
[32mFinished job 1.[0m
[32m1 of 2 steps (50%) done[0m
[32m[0m
[32mlocalrule all:
    input: /mnt/fass1/kirsten/result/classification/clark/default/gridion366_default.clark.classification.csv
    jobid: 0[0m
[32m[0m
[32mFinished job 0.[0m
[32m2 of 2 steps (100%) done[0m
(projectMAIN) [1;34m[[1;32mre85gih[1;34m@[1;32mprost[1;34m [1;34m12:20:27 [1;32m/mnt/fass1/kirsten/result/classificationBenchmark[1;34m][0mexit
exit
